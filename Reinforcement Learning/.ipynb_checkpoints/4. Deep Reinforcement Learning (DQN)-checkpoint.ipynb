{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "1. RL Intro\n",
    "2. Dynamic Programing\n",
    "3. Tabular Q-Learning\n",
    "4. **Deep Reinforcement Learning (DQN)**\n",
    "\n",
    "## 4. Deep Reinforcement Learning (DQN)\n",
    "\n",
    "Welcome to the final Reinforcement Notebook. In this part we will implement the Deep Q-Learning Algorithm that was used by Mnih et al. to play Atari Video games. The resulting agent is called Deep Q-Network agent (or shorter DQN agent) because it uses a Deep Neural Network to approximate the value function (instead of saving it in a table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory\n",
    "\n",
    "In the last notebook we have implemented the model-free **Q-Learning** algorithm and solved the *full reinforcement learning problem* by **learning from samples**. In this context, *full* refered to the fact that we dont have acess to the world model and *model-free* to the fact that we have not tried to learn that model. Furthermore, Q-Learning performed **online** updates to the policy, meaning that we have adjusted the policy *online* after every time step. Finally Q-Learning is an **off-policy** algorithm because we followed an e-greedy behavior policy while we have performed updates according to a greedy target policy. Now we will tackle the **curse of dimensionality** by **approximating the value function** instead of saving it explicitly in a table.\n",
    "\n",
    "### Case Study - Video Games\n",
    "\n",
    "<img src=\"./Atari_games.png\" alt=\"DQN\" style=\"width: 1000px;\"/>\n",
    "\n",
    "Before we proceed to the solution, let us quickly revise the actual problem that we are trying to solve. Consider the task of learning to play a video game given only the raw game screen as input. This is similar to how humans would play the game. Since the game screen is typically represented as raw pixels, this leaves us with a really high dimensional input or state space because every change of pixels represents a new and distinct state of the game, even if the change seems completely insignificant to you! Remember, the agent has no real knowledge of the game (or world model). Clearly it is infeasible to store every possible state of pixle combinations in a table. See [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) chapter 16.5 for a comprehensive discussion.   \n",
    "\n",
    "<img src=\"./DQN_principle.png\" alt=\"DQN\" style=\"width: 800px;\"/>\n",
    "\n",
    "> The problem that is solved by Deep Reinforcement Learning (in the case of DQN) is how to learn a mapping from a high dimensional input space to action values. This mapping represents the value function and can be used in a policy, e.g. to choose the best action with the highest value.\n",
    "\n",
    "\n",
    "### Nonlinear Function Approximation with Artificial Neural Networks\n",
    "\n",
    "First of all, a lookup table can mathematically be seen as a very simple form of a function, i.e. a direct mapping of values (hence the name value function). However, for the reasons explained above, this approach does not scale to high dimensional input spaces. A typical solution to this problem is to replace the *perfect* but intractable lookup table with a more complicated function that *only* **approximates the true value function** but is computationally tractable. In the case of DQN we choose a deep neural network as our function approximator. Formally, this new function is denoted as $\\hat{Q}$ and we write\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "\\hat{Q}(s,a,\\theta) \\approx Q_{\\pi}(s,a)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "where $\\theta$ are the parameters of the neural network. In other words, the value function now depends on those parameters and the task of finding and optimal value function turns into the task of finding an optimal set of parameters for the network. Fortunately, we know how to train and optimize a neural network with SGD and backpropagation given an appropriate loss function. **Inside the RL-framework we can use the TD-error as the loss function**. Formally we optimize:  \n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "L_i(\\theta_{i}) = \\Big( \\underbrace{r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a; \\theta_i) - Q(s_t, a_t;\\theta_i)}_{TD-error} \\Big)^2\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "> Note that in order to obtain any action-values, we now need to perform a forward pass through the network. In practice, this means two forward passes before we can calculate the loss, one pass for the value of $Q(s_t, a_t;\\theta_i)$ and another one to calculate the value of $\\max_a Q(s_{t+1}, a; \\theta_i)$. More details on that later.\n",
    "\n",
    "\n",
    "### Instabilities and Solutions\n",
    "\n",
    "So far, so good. By using the TD-error as loss function we can train the network in a supervised learning like setup. Sadly it is not that easy. Remember that in supervised learning we assumed the data to be *independent and identically distributed* (iid-data) in order for SGD to work properly. This assuption does not hold in reinforcement learning where subsequent data is highly correlated and in contrast, depends strongly on the agents last choice of actions. This inherent sequential property, in combination with an off-policy algorithm and a non-linear function appoximator such as a neural network, results in the problem that the learnable network parameters are at risk to oscillate or even diverge catastrophically during training. In theory, there is no convergence guarantee whatsoever. In practice, Mnih et al. found two major ways in which the training process can be stabilized:\n",
    "\n",
    "- **Experience Replay** - This idea introduces a so called **replay buffer** $\\mathcal{D}$ which stores the last $N$ state transitions as experience tuples $(S,A,R,S')$. In other words, the agent saves its recent history to a buffer. This way, experience can be reused and the correlation between samples can be broken by **drawing random minibatches** of experience $U(\\mathcal{D})$ from it during the training.\n",
    "<br><br>\n",
    "\n",
    "- **Fixed Q-Targets** - The second idea is to keep **a separate set of parameters** $\\theta^{−}$ for calculating the **TD target**. This set is basically a copy of $\\theta$ that is held fixed for some time $t$ and periodically gets swapped with the current parameter values in order to allow progress. Mnih et al. have shown that updating $\\theta$ towards such fixed Q-targets is another effective way to stabilize the training process. In practice, this means that we have basically two separate networks which we will distinguish by their different set of parameters $\\theta^{-}$ and $\\theta$. We will refer to them respectively as **Target-** and **Q-Network**.\n",
    "\n",
    "As a result, the **Q-learning update** of DQN at iteration $i$ uses the following loss function: \n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "L_i(\\theta_{i}) = \\mathbb{E}_{(s,a,r,s') \\sim U(\\mathcal{D})} \\Bigg[ \\Bigg( r_{t+1} + \\gamma \\max_a \\underbrace{Q(s_{t+1}, a; \\theta^{-}_{i})}_{Target-Network} - \\underbrace{Q(s_t, a_t;\\theta_i)}_{Q-Network} \\Big)^2 \\Bigg]\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "And thats it! We can use this update rule inside the Q-Learning algorithm to train a Deep Q-Network with SGD as we do in supervised learning. The corresponding **Deep Q-Learning algorithm** is given in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "As in the previous notebooks we will implement the DQN algorithm step by step. While the original DQN architecture was a CNN trained on Atari games, we will choose a much simpler problem and architecture. This way you can verify and debug your implementation much faster (in minutes vs hours...). However, the algorithm itself is still the same and extending it should be straightforward.\n",
    "\n",
    "The following is an overview of all the parts we need. We use it as a checklist. Like with Q-Learning, we try first to verify that all the sub-parts are working as expected.\n",
    "\n",
    "##### Overview\n",
    "\n",
    "- The OpenAI Gym Environment\n",
    "- Replay Buffer\n",
    "- Epsilon Schedule\n",
    "- Deep Q-Network (and computation graph)\n",
    "- E-Greedy Policy (action selection)\n",
    "- Update the Target Network\n",
    "- Train Method\n",
    "- Main Loop\n",
    "<br><br>\n",
    "- Evaluation of Deep Reinforcement Learning Algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Open AI Gym Environment\n",
    "\n",
    "We will use the OpenAI Gym environment to solve a classic control task known as **Cart Pole Balance**. The great thing about the gym environment is that it offers a common interface to [many different environments](https://gym.openai.com/envs/#classic_control). That way you can easily test your algorithms on different tasks, e.g. switch from an easy one like CartPole to more challenging ones like an Atari game etc. ;)\n",
    "\n",
    "For now, we will solve the **CartPole-v0** task. \n",
    "\n",
    "1. First of all, [we go and read](https://github.com/openai/gym/wiki/CartPole-v0) about its most important details such as the *observations, actions, rewards,* it's *max length* etc.\n",
    "2. Second, we get used to the gym interface. We Run a random agent for some episodes etc. The most important API calls are:\n",
    "    - `gym.make('CartPole-v0')` returns a new `game`.\n",
    "    - The game's `action_space` and `observation_space` variables.\n",
    "    - `reset()` - returns an initial `observation`.\n",
    "    - `step()`  - takes an `action` int, returns an `observation, reward, game_over, info` tuple.\n",
    "    - `render()` - renders the current game state.\n",
    "    - `close()` - call this after the last episode has ended to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02577703 -0.15628974  0.00464515  0.29117978] 1.0 False {}\n",
      "[ 0.02265124 -0.35147762  0.01046875  0.58532411] 1.0 False {}\n",
      "[ 0.01562169 -0.54674463  0.02217523  0.88128633] 1.0 False {}\n",
      "[ 0.00468679 -0.74216067  0.03980096  1.18085731] 1.0 False {}\n",
      "[-0.01015642 -0.93777603  0.0634181   1.48574636] 1.0 False {}\n",
      "[-0.02891194 -1.13361102  0.09313303  1.79754075] 1.0 False {}\n",
      "[-0.05158416 -0.9396466   0.12908385  1.53519656] 1.0 False {}\n",
      "[-0.07037709 -1.13606517  0.15978778  1.86521853] 1.0 False {}\n",
      "[-0.0930984  -1.33253593  0.19709215  2.20294838] 1.0 False {}\n",
      "[-0.11974912 -1.13978162  0.24115111  1.97698514] 1.0 True {}\n",
      "Episode finished after 9 steps\n",
      "[ 0.00478715 -0.23503177 -0.01853886  0.25410996] 1.0 False {}\n",
      "[ 8.65147100e-05 -4.29884171e-01 -1.34566641e-02  5.40888233e-01] 1.0 False {}\n",
      "[-0.00851117 -0.62481442 -0.0026389   0.82930099] 1.0 False {}\n",
      "[-0.02100746 -0.42965649  0.01394712  0.53578928] 1.0 False {}\n",
      "[-0.02960059 -0.2347334   0.02466291  0.24753341] 1.0 False {}\n",
      "[-0.03429525 -0.0399722   0.02961357 -0.03726939] 1.0 False {}\n",
      "[-0.0350947   0.15471285  0.02886819 -0.3204638 ] 1.0 False {}\n",
      "[-0.03200044  0.34941204  0.02245891 -0.60390484] 1.0 False {}\n",
      "[-0.0250122   0.54421281  0.01038081 -0.88942994] 1.0 False {}\n",
      "[-0.01412795  0.34895154 -0.00740779 -0.59350192] 1.0 False {}\n",
      "[-0.00714891  0.5441764  -0.01927782 -0.88850903] 1.0 False {}\n",
      "[ 0.00373461  0.3493213  -0.037048   -0.60194803] 1.0 False {}\n",
      "[ 0.01072104  0.54494136 -0.04908696 -0.90606638] 1.0 False {}\n",
      "[ 0.02161987  0.74069237 -0.06720829 -1.213765  ] 1.0 False {}\n",
      "[ 0.03643371  0.54649895 -0.09148359 -0.94287685] 1.0 False {}\n",
      "[ 0.04736369  0.74272653 -0.11034113 -1.26284606] 1.0 False {}\n",
      "[ 0.06221822  0.54917462 -0.13559805 -1.0066583 ] 1.0 False {}\n",
      "[ 0.07320172  0.35609812 -0.15573122 -0.75944579] 1.0 False {}\n",
      "[ 0.08032368  0.16342552 -0.17092013 -0.51953451] 1.0 False {}\n",
      "[ 0.08359219 -0.02893025 -0.18131082 -0.28521138] 1.0 False {}\n",
      "[ 0.08301358 -0.22106574 -0.18701505 -0.05474612] 1.0 False {}\n",
      "[ 0.07859227 -0.41308257 -0.18810997  0.17359452] 1.0 False {}\n",
      "[ 0.07033062 -0.60508415 -0.18463808  0.40153762] 1.0 False {}\n",
      "[ 0.05822894 -0.40788921 -0.17660733  0.05679716] 1.0 False {}\n",
      "[ 0.05007115 -0.21073263 -0.17547139 -0.28599073] 1.0 False {}\n",
      "[ 0.0458565  -0.40297465 -0.1811912  -0.05337996] 1.0 False {}\n",
      "[ 0.03779701 -0.59509857 -0.1822588   0.17710902] 1.0 False {}\n",
      "[ 0.02589503 -0.78720791 -0.17871662  0.4072122 ] 1.0 False {}\n",
      "[ 0.01015088 -0.97940538 -0.17057238  0.63865318] 1.0 False {}\n",
      "[-0.00943723 -1.17179019 -0.15779931  0.87313703] 1.0 False {}\n",
      "[-0.03287304 -0.97491516 -0.14033657  0.5352929 ] 1.0 False {}\n",
      "[-0.05237134 -0.77812773 -0.12963071  0.20189002] 1.0 False {}\n",
      "[-0.06793389 -0.97118043 -0.12559291  0.45103465] 1.0 False {}\n",
      "[-0.0873575  -0.77452682 -0.11657222  0.1215503 ] 1.0 False {}\n",
      "[-0.10284804 -0.96780243 -0.11414121  0.37530144] 1.0 False {}\n",
      "[-0.12220409 -1.16113376 -0.10663519  0.62992672] 1.0 False {}\n",
      "[-0.14542676 -1.3546188  -0.09403665  0.88721438] 1.0 False {}\n",
      "[-0.17251914 -1.15835488 -0.07629236  0.56651316] 1.0 False {}\n",
      "[-0.19568624 -1.35232835 -0.0649621   0.83421879] 1.0 False {}\n",
      "[-0.2227328  -1.54650546 -0.04827772  1.10578468] 1.0 False {}\n",
      "[-0.25366291 -1.35078314 -0.02616203  0.79835512] 1.0 False {}\n",
      "[-0.28067857 -1.54553657 -0.01019493  1.08269454] 1.0 False {}\n",
      "[-0.31158931 -1.35028157  0.01145896  0.78683   ] 1.0 False {}\n",
      "[-0.33859494 -1.15531889  0.02719556  0.49777404] 1.0 False {}\n",
      "[-0.36170132 -0.96059073  0.03715104  0.21378433] 1.0 False {}\n",
      "[-0.38091313 -1.15622359  0.04142673  0.51795102] 1.0 False {}\n",
      "[-0.4040376  -0.96170865  0.05178575  0.23860492] 1.0 False {}\n",
      "[-0.42327177 -0.76736328  0.05655785 -0.03730412] 1.0 False {}\n",
      "[-0.43861904 -0.57309607  0.05581177 -0.31161986] 1.0 False {}\n",
      "[-0.45008096 -0.37881183  0.04957937 -0.5861923 ] 1.0 False {}\n",
      "[-0.4576572  -0.57459186  0.03785552 -0.27831276] 1.0 False {}\n",
      "[-0.46914904 -0.77023283  0.03228927  0.02606532] 1.0 False {}\n",
      "[-0.48455369 -0.9658026   0.03281057  0.32875846] 1.0 False {}\n",
      "[-0.50386974 -0.77116273  0.03938574  0.04660037] 1.0 False {}\n",
      "[-0.519293   -0.96682667  0.04031775  0.35144507] 1.0 False {}\n",
      "[-0.53862953 -0.77230057  0.04734665  0.07174326] 1.0 False {}\n",
      "[-0.55407554 -0.57788826  0.04878152 -0.2056337 ] 1.0 False {}\n",
      "[-0.56563331 -0.3834966   0.04466884 -0.48253823] 1.0 False {}\n",
      "[-0.57330324 -0.57921962  0.03501808 -0.17611828] 1.0 False {}\n",
      "[-0.58488763 -0.38461586  0.03149571 -0.45755175] 1.0 False {}\n",
      "[-0.59257995 -0.189953    0.02234468 -0.74014302] 1.0 False {}\n",
      "[-0.59637901  0.00485343  0.00754182 -1.02571093] 1.0 False {}\n",
      "[-0.59628194 -0.19036813 -0.0129724  -0.73066965] 1.0 False {}\n",
      "[-0.6000893   0.00493069 -0.02758579 -1.027407  ] 1.0 False {}\n",
      "[-0.59999069 -0.18981341 -0.04813393 -0.74351119] 1.0 False {}\n",
      "[-0.60378696 -0.38423913 -0.06300416 -0.4663563 ] 1.0 False {}\n",
      "[-0.61147174 -0.57841693 -0.07233128 -0.19417847] 1.0 False {}\n",
      "[-0.62304008 -0.38233885 -0.07621485 -0.50877341] 1.0 False {}\n",
      "[-0.63068686 -0.18623051 -0.08639032 -0.82446746] 1.0 False {}\n",
      "[-0.63441147  0.00996016 -0.10287967 -1.14302314] 1.0 False {}\n",
      "[-0.63421226 -0.18367821 -0.12574013 -0.88429467] 1.0 False {}\n",
      "[-0.63788583  0.01290619 -0.14342603 -1.21371352] 1.0 False {}\n",
      "[-0.6376277   0.20955759 -0.1677003  -1.54768466] 1.0 False {}\n",
      "[-0.63343655  0.40624874 -0.19865399 -1.88765548] 1.0 False {}\n",
      "[-0.62531158  0.21376389 -0.2364071  -1.66262748] 1.0 True {}\n",
      "Episode finished after 74 steps\n",
      "[ 0.01789803  0.1982308   0.01697819 -0.27282425] 1.0 False {}\n",
      "[0.02186264 0.00287076 0.01152171 0.02516493] 1.0 False {}\n",
      "[ 0.02192006 -0.19241451  0.012025    0.3214607 ] 1.0 False {}\n",
      "[ 0.01807177 -0.38770562  0.01845422  0.61791145] 1.0 False {}\n",
      "[ 0.01031765 -0.58308043  0.03081245  0.916349  ] 1.0 False {}\n",
      "[-0.00134395 -0.38838838  0.04913943  0.63350697] 1.0 False {}\n",
      "[-0.00911172 -0.58416013  0.06180957  0.9412513 ] 1.0 False {}\n",
      "[-0.02079492 -0.78005819  0.08063459  1.25269769] 1.0 False {}\n",
      "[-0.03639609 -0.58605643  0.10568855  0.98632147] 1.0 False {}\n",
      "[-0.04811722 -0.78242282  0.12541498  1.31024215] 1.0 False {}\n",
      "[-0.06376567 -0.97889018  0.15161982  1.63940396] 1.0 False {}\n",
      "[-0.08334348 -0.78583572  0.1844079   1.39755007] 1.0 False {}\n",
      "[-0.09906019 -0.59342286  0.2123589   1.1677335 ] 1.0 True {}\n",
      "Episode finished after 12 steps\n",
      "[ 0.02154979  0.215784   -0.03144584 -0.34133608] 1.0 False {}\n",
      "[ 0.02586547  0.02112324 -0.03827256 -0.05873284] 1.0 False {}\n",
      "[ 0.02628794 -0.17342965 -0.03944721  0.2216334 ] 1.0 False {}\n",
      "[ 0.02281934  0.02223329 -0.03501455 -0.08322716] 1.0 False {}\n",
      "[ 0.02326401 -0.1723697  -0.03667909  0.19820623] 1.0 False {}\n",
      "[ 0.01981662  0.02325716 -0.03271496 -0.10581792] 1.0 False {}\n",
      "[ 0.02028176  0.21883229 -0.03483132 -0.40864013] 1.0 False {}\n",
      "[ 0.02465841  0.02422106 -0.04300413 -0.12713869] 1.0 False {}\n",
      "[ 0.02514283 -0.17025929 -0.0455469   0.15167281] 1.0 False {}\n",
      "[ 0.02173764  0.02548425 -0.04251344 -0.15502416] 1.0 False {}\n",
      "[ 0.02224733 -0.16900402 -0.04561393  0.12394946] 1.0 False {}\n",
      "[ 0.01886725 -0.36344381 -0.04313494  0.40189994] 1.0 False {}\n",
      "[ 0.01159837 -0.55792824 -0.03509694  0.6806775 ] 1.0 False {}\n",
      "[ 0.0004398  -0.36233684 -0.02148339  0.3771547 ] 1.0 False {}\n",
      "[-0.00680693 -0.16691646 -0.01394029  0.07777614] 1.0 False {}\n",
      "[-0.01014526 -0.36183583 -0.01238477  0.36602846] 1.0 False {}\n",
      "[-0.01738198 -0.55677961 -0.0050642   0.65478068] 1.0 False {}\n",
      "[-0.02851757 -0.75183069  0.00803141  0.94586468] 1.0 False {}\n",
      "[-0.04355418 -0.94705989  0.0269487   1.24106024] 1.0 False {}\n",
      "[-0.06249538 -1.14251726  0.05176991  1.54206181] 1.0 False {}\n",
      "[-0.08534573 -1.33822195  0.08261115  1.85043909] 1.0 False {}\n",
      "[-0.11211017 -1.14410044  0.11961993  1.58450993] 1.0 False {}\n",
      "[-0.13499217 -1.34042472  0.15131013  1.91197642] 1.0 False {}\n",
      "[-0.16180067 -1.53681962  0.18954965  2.24751851] 1.0 False {}\n",
      "[-0.19253706 -1.34392397  0.23450002  2.0187519 ] 1.0 True {}\n",
      "Episode finished after 24 steps\n",
      "[ 0.00260014 -0.22236927 -0.00057109  0.31970999] 1.0 False {}\n",
      "[-0.00184724 -0.41748309  0.00582311  0.61221276] 1.0 False {}\n",
      "[-0.0101969  -0.61268593  0.01806736  0.90672405] 1.0 False {}\n",
      "[-0.02245062 -0.80804778  0.03620185  1.20503052] 1.0 False {}\n",
      "[-0.03861158 -1.00361845  0.06030246  1.50883532] 1.0 False {}\n",
      "[-0.05868395 -1.19941724  0.09047916  1.81971785] 1.0 False {}\n",
      "[-0.08267229 -1.00540981  0.12687352  1.55646167] 1.0 False {}\n",
      "[-0.10278049 -0.81201546  0.15800275  1.30590261] 1.0 False {}\n",
      "[-0.1190208  -1.00874769  0.1841208   1.64358485] 1.0 False {}\n",
      "[-0.13919575 -0.81619677  0.2169925   1.41346651] 1.0 True {}\n",
      "Episode finished after 9 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02188817  0.19280135 -0.0263784  -0.29133152] 1.0 False {}\n",
      "[ 0.0257442  -0.00193474 -0.03220503 -0.00708335] 1.0 False {}\n",
      "[ 0.02570551  0.19363392 -0.0323467  -0.30975087] 1.0 False {}\n",
      "[ 0.02957818 -0.00101259 -0.03854172 -0.02744211] 1.0 False {}\n",
      "[ 0.02955793  0.19464028 -0.03909056 -0.33203192] 1.0 False {}\n",
      "[ 3.34507387e-02  9.59095917e-05 -4.57311970e-02 -5.19279930e-02] 1.0 False {}\n",
      "[ 0.03345266 -0.19434149 -0.04676976  0.2259829 ] 1.0 False {}\n",
      "[ 0.02956583 -0.38876491 -0.0422501   0.50355382] 1.0 False {}\n",
      "[ 0.02179053 -0.58326671 -0.03217902  0.78262833] 1.0 False {}\n",
      "[ 0.01012519 -0.38771762 -0.01652646  0.47999755] 1.0 False {}\n",
      "[ 0.00237084 -0.19236632 -0.0069265   0.18215205] 1.0 False {}\n",
      "[-0.00147648 -0.38738847 -0.00328346  0.4726419 ] 1.0 False {}\n",
      "[-0.00922425 -0.5824639   0.00616937  0.76428812] 1.0 False {}\n",
      "[-0.02087353 -0.38742745  0.02145514  0.4735528 ] 1.0 False {}\n",
      "[-0.02862208 -0.19261497  0.03092619  0.18770866] 1.0 False {}\n",
      "[-0.03247438 -0.38816542  0.03468037  0.48998492] 1.0 False {}\n",
      "[-0.04023769 -0.58375901  0.04448006  0.79339288] 1.0 False {}\n",
      "[-0.05191287 -0.38927496  0.06034792  0.51502816] 1.0 False {}\n",
      "[-0.05969837 -0.19505247  0.07064848  0.24195629] 1.0 False {}\n",
      "[-0.06359942 -0.39110874  0.07548761  0.55606047] 1.0 False {}\n",
      "[-0.07142159 -0.58720483  0.08660882  0.87153921] 1.0 False {}\n",
      "[-0.08316569 -0.3933608   0.1040396   0.60729419] 1.0 False {}\n",
      "[-0.0910329  -0.58977172  0.11618549  0.93085   ] 1.0 False {}\n",
      "[-0.10282834 -0.78625379  0.13480249  1.25766783] 1.0 False {}\n",
      "[-0.11855341 -0.59308966  0.15995584  1.01006226] 1.0 False {}\n",
      "[-0.13041521 -0.40042188  0.18015709  0.77157663] 1.0 False {}\n",
      "[-0.13842365 -0.20817568  0.19558862  0.54055455] 1.0 False {}\n",
      "[-0.14258716 -0.40543112  0.20639971  0.88793336] 1.0 False {}\n",
      "[-0.15069578 -0.21361772  0.22415838  0.66657168] 1.0 True {}\n",
      "Episode finished after 28 steps\n",
      "[-0.005133   -0.21132825 -0.01416758  0.2718297 ] 1.0 False {}\n",
      "[-0.00935956 -0.01600703 -0.00873098 -0.02528785] 1.0 False {}\n",
      "[-0.0096797   0.17923903 -0.00923674 -0.32071266] 1.0 False {}\n",
      "[-0.00609492 -0.01575017 -0.01565099 -0.0309569 ] 1.0 False {}\n",
      "[-0.00640992 -0.21064422 -0.01627013  0.25674716] 1.0 False {}\n",
      "[-0.01062281 -0.40553014 -0.01113519  0.54425416] 1.0 False {}\n",
      "[-1.87334105e-02 -6.00493862e-01 -2.50105226e-04  8.33407925e-01] 1.0 False {}\n",
      "[-0.03074329 -0.40536849  0.01641805  0.54064635] 1.0 False {}\n",
      "[-0.03885066 -0.21048112  0.02723098  0.25318138] 1.0 False {}\n",
      "[-0.04306028 -0.01575837  0.03229461 -0.03078954] 1.0 False {}\n",
      "[-0.04337545  0.17888593  0.03167882 -0.31311079] 1.0 False {}\n",
      "[-0.03979773 -0.01667265  0.0254166  -0.01060808] 1.0 False {}\n",
      "[-0.04013118  0.17807575  0.02520444 -0.29516465] 1.0 False {}\n",
      "[-0.03656967  0.37282947  0.01930115 -0.57979313] 1.0 False {}\n",
      "[-0.02911308  0.56767571  0.00770528 -0.86633386] 1.0 False {}\n",
      "[-0.01775956  0.76269195 -0.00962139 -1.15658421] 1.0 False {}\n",
      "[-0.00250572  0.957938   -0.03275308 -1.45226837] 1.0 False {}\n",
      "[ 0.01665304  1.15344662 -0.06179844 -1.7550017 ] 1.0 False {}\n",
      "[ 0.03972197  0.95907736 -0.09689848 -1.48216154] 1.0 False {}\n",
      "[ 0.05890352  0.76526178 -0.12654171 -1.22124554] 1.0 False {}\n",
      "[ 0.07420875  0.96176657 -0.15096662 -1.55074998] 1.0 False {}\n",
      "[ 0.09344408  0.76874355 -0.18198162 -1.30872436] 1.0 False {}\n",
      "[ 0.10881895  0.57633296 -0.20815611 -1.07808216] 1.0 False {}\n",
      "[ 0.12034561  0.77350422 -0.22971775 -1.42821164] 1.0 True {}\n",
      "Episode finished after 23 steps\n",
      "[-0.04697303  0.23168493  0.04145007 -0.28809758] 1.0 False {}\n",
      "[-0.04233933  0.03599715  0.03568812  0.01736481] 1.0 False {}\n",
      "[-0.04161939 -0.15961795  0.03603542  0.32109071] 1.0 False {}\n",
      "[-0.04481174  0.03497279  0.04245723  0.03998621] 1.0 False {}\n",
      "[-0.04411229  0.229461    0.04325696 -0.23900453] 1.0 False {}\n",
      "[-0.03952307  0.42393916  0.03847687 -0.5177353 ] 1.0 False {}\n",
      "[-0.03104429  0.61849883  0.02812216 -0.79804941] 1.0 False {}\n",
      "[-0.01867431  0.42300256  0.01216117 -0.49665412] 1.0 False {}\n",
      "[-0.01021426  0.22771126  0.00222809 -0.20016354] 1.0 False {}\n",
      "[-0.00566003  0.03255751 -0.00177518  0.09322142] 1.0 False {}\n",
      "[-5.00888205e-03  2.27704861e-01  8.92470545e-05 -2.00021050e-01] 1.0 False {}\n",
      "[-0.00045478  0.03258163 -0.00391117  0.09269003] 1.0 False {}\n",
      "[ 1.96847840e-04  2.27759425e-01 -2.05737336e-03 -2.01224301e-01] 1.0 False {}\n",
      "[ 0.00475204  0.03266696 -0.00608186  0.09080891] 1.0 False {}\n",
      "[ 0.00540538  0.22787555 -0.00426568 -0.20378662] 1.0 False {}\n",
      "[ 0.00996289  0.03281486 -0.00834141  0.08754764] 1.0 False {}\n",
      "[ 0.01061918  0.22805538 -0.00659046 -0.20775529] 1.0 False {}\n",
      "[ 0.01518029  0.42327095 -0.01074557 -0.50250986] 1.0 False {}\n",
      "[ 0.02364571  0.6185427  -0.02079576 -0.79855971] 1.0 False {}\n",
      "[ 0.03601656  0.42371213 -0.03676696 -0.51249056] 1.0 False {}\n",
      "[ 0.04449081  0.22912682 -0.04701677 -0.2316169 ] 1.0 False {}\n",
      "[ 0.04907334  0.42488796 -0.05164911 -0.53875196] 1.0 False {}\n",
      "[ 0.0575711   0.2305287  -0.06242415 -0.26277993] 1.0 False {}\n",
      "[ 0.06218168  0.03635077 -0.06767975  0.00957887] 1.0 False {}\n",
      "[ 0.06290869 -0.15773857 -0.06748817  0.2801637 ] 1.0 False {}\n",
      "[ 0.05975392 -0.35183615 -0.06188489  0.55082083] 1.0 False {}\n",
      "[ 0.0527172  -0.5460368  -0.05086848  0.82338163] 1.0 False {}\n",
      "[ 0.04179646 -0.74042734 -0.03440084  1.09964138] 1.0 False {}\n",
      "[ 0.02698791 -0.54486992 -0.01240802  0.79636695] 1.0 False {}\n",
      "[ 0.01609052 -0.34957992  0.00351932  0.49980663] 1.0 False {}\n",
      "[ 0.00909892 -0.15450776  0.01351545  0.20823488] 1.0 False {}\n",
      "[ 0.00600876  0.04041835  0.01768015 -0.08015416] 1.0 False {}\n",
      "[ 0.00681713 -0.15495253  0.01607707  0.21805405] 1.0 False {}\n",
      "[ 0.00371808  0.03993596  0.02043815 -0.06951444] 1.0 False {}\n",
      "[ 0.0045168   0.23475902  0.01904786 -0.3556796 ] 1.0 False {}\n",
      "[ 0.00921198  0.0393715   0.01193427 -0.05705175] 1.0 False {}\n",
      "[ 0.00999941  0.23432031  0.01079323 -0.34594556] 1.0 False {}\n",
      "[ 0.01468581  0.03904651  0.00387432 -0.04987877] 1.0 False {}\n",
      "[ 0.01546675  0.23411269  0.00287675 -0.3413368 ] 1.0 False {}\n",
      "[ 0.020149    0.03894993 -0.00394999 -0.0477481 ] 1.0 False {}\n",
      "[ 0.020928    0.2341283  -0.00490495 -0.34167466] 1.0 False {}\n",
      "[ 0.02561056  0.03907647 -0.01173844 -0.0505425 ] 1.0 False {}\n",
      "[ 0.02639209  0.23436476 -0.01274929 -0.34690576] 1.0 False {}\n",
      "[ 0.03107939  0.03942645 -0.01968741 -0.05827026] 1.0 False {}\n",
      "[ 0.03186792  0.23482507 -0.02085281 -0.35709911] 1.0 False {}\n",
      "[ 0.03656442  0.0400057  -0.0279948  -0.07106387] 1.0 False {}\n",
      "[ 0.03736453 -0.15470395 -0.02941607  0.21265677] 1.0 False {}\n",
      "[ 0.03427045  0.04082595 -0.02516294 -0.08915828] 1.0 False {}\n",
      "[ 0.03508697 -0.15392645 -0.0269461   0.19548073] 1.0 False {}\n",
      "[ 0.03200844  0.04157037 -0.02303649 -0.10557925] 1.0 False {}\n",
      "[ 0.03283985  0.23701474 -0.02514807 -0.40544015] 1.0 False {}\n",
      "[ 0.03758015  0.43248412 -0.03325688 -0.70594427] 1.0 False {}\n",
      "[ 0.04622983  0.62805069 -0.04737576 -1.00890762] 1.0 False {}\n",
      "[ 0.05879084  0.82377191 -0.06755391 -1.31608332] 1.0 False {}\n",
      "[ 0.07526628  1.01968036 -0.09387558 -1.62912146] 1.0 False {}\n",
      "[ 0.09565989  1.21577193 -0.12645801 -1.94952262] 1.0 False {}\n",
      "[ 0.11997533  1.02220195 -0.16544846 -1.69856582] 1.0 False {}\n",
      "[ 0.14041936  0.82932893 -0.19941978 -1.46162716] 1.0 False {}\n",
      "[ 0.15700594  1.02625726 -0.22865232 -1.80940706] 1.0 True {}\n",
      "Episode finished after 58 steps\n",
      "[ 0.02911303  0.21183469 -0.01232176 -0.26659262] 1.0 False {}\n",
      "[ 0.03334973  0.40713031 -0.01765361 -0.56313632] 1.0 False {}\n",
      "[ 0.04149233  0.21226046 -0.02891633 -0.27606699] 1.0 False {}\n",
      "[ 0.04573754  0.01756274 -0.03443767  0.00735728] 1.0 False {}\n",
      "[ 0.0460888   0.21316122 -0.03429053 -0.29598915] 1.0 False {}\n",
      "[ 0.05035202  0.01854447 -0.04021031 -0.01431507] 1.0 False {}\n",
      "[ 0.05072291  0.21421933 -0.04049661 -0.31940875] 1.0 False {}\n",
      "[ 0.0550073   0.40989393 -0.04688479 -0.62458276] 1.0 False {}\n",
      "[ 0.06320518  0.2154568  -0.05937644 -0.34702664] 1.0 False {}\n",
      "[ 0.06751431  0.41137085 -0.06631698 -0.65782625] 1.0 False {}\n",
      "[ 0.07574173  0.2172316  -0.0794735  -0.3867404 ] 1.0 False {}\n",
      "[ 0.08008636  0.41338647 -0.08720831 -0.70338464] 1.0 False {}\n",
      "[ 0.08835409  0.60960183 -0.101276   -1.02219594] 1.0 False {}\n",
      "[ 0.10054613  0.41596418 -0.12171992 -0.76295204] 1.0 False {}\n",
      "[ 0.10886541  0.61253358 -0.13697896 -1.09132195] 1.0 False {}\n",
      "[ 0.12111608  0.80916859 -0.1588054  -1.42365765] 1.0 False {}\n",
      "[ 0.13729945  1.00585733 -0.18727855 -1.76147113] 1.0 False {}\n",
      "[ 0.1574166   0.81328372 -0.22250798 -1.53240017] 1.0 True {}\n",
      "Episode finished after 17 steps\n",
      "[-0.04385769 -0.21537465  0.00317651  0.32129183] 1.0 False {}\n",
      "[-0.04816519 -0.41054169  0.00960235  0.61497481] 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05637602 -0.21555522  0.02190184  0.32533163] 1.0 False {}\n",
      "[-0.06068713 -0.41098205  0.02840848  0.6248402 ] 1.0 False {}\n",
      "[-0.06890677 -0.21626798  0.04090528  0.34123791] 1.0 False {}\n",
      "[-0.07323213 -0.02175117  0.04773004  0.06172956] 1.0 False {}\n",
      "[-0.07366715  0.1726551   0.04896463 -0.21552043] 1.0 False {}\n",
      "[-0.07021405 -0.02313142  0.04465422  0.09219721] 1.0 False {}\n",
      "[-0.07067668 -0.21886402  0.04649817  0.39862742] 1.0 False {}\n",
      "[-0.07505396 -0.02443149  0.05447071  0.12095939] 1.0 False {}\n",
      "[-0.07554259 -0.22028983  0.0568899   0.43031764] 1.0 False {}\n",
      "[-0.07994838 -0.02601769  0.06549626  0.15609747] 1.0 False {}\n",
      "[-0.08046874  0.16810835  0.0686182  -0.1152251 ] 1.0 False {}\n",
      "[-0.07710657  0.36218343  0.0663137  -0.38549472] 1.0 False {}\n",
      "[-0.0698629   0.16618581  0.05860381 -0.07266254] 1.0 False {}\n",
      "[-0.06653918  0.36042076  0.05715056 -0.34629514] 1.0 False {}\n",
      "[-0.05933077  0.16453442  0.05022465 -0.03615223] 1.0 False {}\n",
      "[-0.05604008  0.35890151  0.04950161 -0.31257537] 1.0 False {}\n",
      "[-0.04886205  0.55328458  0.0432501  -0.58924529] 1.0 False {}\n",
      "[-0.03779636  0.74777508  0.0314652  -0.86799666] 1.0 False {}\n",
      "[-0.02284086  0.9424551   0.01410526 -1.1506229 ] 1.0 False {}\n",
      "[-0.00399176  0.74715195 -0.00890719 -0.8535505 ] 1.0 False {}\n",
      "[ 0.01095128  0.55215253 -0.0259782  -0.56368166] 1.0 False {}\n",
      "[ 0.02199433  0.74762918 -0.03725184 -0.86443443] 1.0 False {}\n",
      "[ 0.03694692  0.94323788 -0.05454053 -1.16859342] 1.0 False {}\n",
      "[ 0.05581168  1.13902527 -0.07791239 -1.47786478] 1.0 False {}\n",
      "[ 0.07859218  1.33500736 -0.10746969 -1.79382919] 1.0 False {}\n",
      "[ 0.10529233  1.14124123 -0.14334627 -1.53639214] 1.0 False {}\n",
      "[ 0.12811715  1.33776848 -0.17407412 -1.87015911] 1.0 False {}\n",
      "[ 0.15487252  1.14492517 -0.2114773  -1.63618544] 1.0 True {}\n",
      "Episode finished after 29 steps\n",
      "[-0.04325383  0.24345116  0.01844033 -0.3113003 ] 1.0 False {}\n",
      "[-0.0383848   0.4383056   0.01221433 -0.59811112] 1.0 False {}\n",
      "[-2.96186917e-02  2.43014890e-01  2.52106012e-04 -3.01605987e-01] 1.0 False {}\n",
      "[-0.02475839  0.43813325 -0.00578001 -0.59420939] 1.0 False {}\n",
      "[-0.01599573  0.63333562 -0.0176642  -0.88870738] 1.0 False {}\n",
      "[-0.00332902  0.43845779 -0.03543835 -0.60162924] 1.0 False {}\n",
      "[ 0.00544014  0.63405707 -0.04747093 -0.90526064] 1.0 False {}\n",
      "[ 0.01812128  0.82978861 -0.06557615 -1.21247842] 1.0 False {}\n",
      "[ 0.03471705  1.02569279 -0.08982572 -1.52496866] 1.0 False {}\n",
      "[ 0.05523091  1.22177708 -0.12032509 -1.84428255] 1.0 False {}\n",
      "[ 0.07966645  1.41800286 -0.15721074 -2.17178332] 1.0 False {}\n",
      "[ 0.10802651  1.61427001 -0.20064641 -2.50858325] 1.0 False {}\n",
      "[ 0.14031191  1.81039925 -0.25081807 -2.85547001] 1.0 True {}\n",
      "Episode finished after 12 steps\n",
      "[-4.07895905e-02  1.51808341e-01  2.20265176e-04 -3.08200057e-01] 1.0 False {}\n",
      "[-0.03775342  0.34692715 -0.00594374 -0.60081351] 1.0 False {}\n",
      "[-0.03081488  0.15188885 -0.01796001 -0.31000868] 1.0 False {}\n",
      "[-0.0277771   0.34726203 -0.02416018 -0.60830113] 1.0 False {}\n",
      "[-0.02083186  0.54271327 -0.0363262  -0.90849483] 1.0 False {}\n",
      "[-0.0099776   0.73830761 -0.0544961  -1.21237034] 1.0 False {}\n",
      "[ 0.00478855  0.93408896 -0.07874351 -1.52162032] 1.0 False {}\n",
      "[ 0.02347033  1.13006913 -0.10917591 -1.83780633] 1.0 False {}\n",
      "[ 0.04607172  0.93630978 -0.14593204 -1.58093171] 1.0 False {}\n",
      "[ 0.06479791  1.13283638 -0.17755067 -1.91534012] 1.0 False {}\n",
      "[ 0.08745464  0.94001557 -0.21585748 -1.68258191] 1.0 True {}\n",
      "Episode finished after 10 steps\n",
      "[ 0.04697709  0.15711208 -0.0293109  -0.34529637] 1.0 False {}\n",
      "[ 0.05011933 -0.03758092 -0.03621683 -0.06199847] 1.0 False {}\n",
      "[ 0.04936771 -0.23216539 -0.03745679  0.21904142] 1.0 False {}\n",
      "[ 0.04472441 -0.03652858 -0.03307597 -0.08521768] 1.0 False {}\n",
      "[ 0.04399384  0.15905149 -0.03478032 -0.3881499 ] 1.0 False {}\n",
      "[ 0.04717486  0.35464943 -0.04254332 -0.69159271] 1.0 False {}\n",
      "[ 0.05426785  0.55033503 -0.05637517 -0.99735948] 1.0 False {}\n",
      "[ 0.06527455  0.35601033 -0.07632236 -0.72290103] 1.0 False {}\n",
      "[ 0.07239476  0.16202235 -0.09078038 -0.45518315] 1.0 False {}\n",
      "[ 0.07563521 -0.03170664 -0.09988405 -0.19243903] 1.0 False {}\n",
      "[ 0.07500107 -0.22526839 -0.10373283  0.06713934] 1.0 False {}\n",
      "[ 0.07049571 -0.41876196 -0.10239004  0.32537675] 1.0 False {}\n",
      "[ 0.06212047 -0.61228835 -0.0958825   0.58409591] 1.0 False {}\n",
      "[ 0.0498747  -0.41596324 -0.08420059  0.26281461] 1.0 False {}\n",
      "[ 0.04155544 -0.21974665 -0.07894429 -0.05519329] 1.0 False {}\n",
      "[ 0.0371605  -0.41365311 -0.08004816  0.21157501] 1.0 False {}\n",
      "[ 0.02888744 -0.21748332 -0.07581666 -0.10524646] 1.0 False {}\n",
      "[ 0.02453777 -0.02136136 -0.07792159 -0.42085305] 1.0 False {}\n",
      "[ 0.02411055 -0.21529779 -0.08633865 -0.1537169 ] 1.0 False {}\n",
      "[ 0.01980459 -0.40908421 -0.08941299  0.11052796] 1.0 False {}\n",
      "[ 0.01162291 -0.21280235 -0.08720243 -0.20897112] 1.0 False {}\n",
      "[ 0.00736686 -0.40657619 -0.09138185  0.05498018] 1.0 False {}\n",
      "[-0.00076466 -0.21027085 -0.09028225 -0.26507811] 1.0 False {}\n",
      "[-0.00497008 -0.403996   -0.09558381 -0.00218079] 1.0 False {}\n",
      "[-0.01305    -0.20764244 -0.09562743 -0.32342558] 1.0 False {}\n",
      "[-0.01720285 -0.40128187 -0.10209594 -0.06236512] 1.0 False {}\n",
      "[-0.02522849 -0.20485555 -0.10334324 -0.38543442] 1.0 False {}\n",
      "[-0.0293256  -0.3983702  -0.11105193 -0.12703995] 1.0 False {}\n",
      "[-0.037293   -0.20184702 -0.11359273 -0.45259107] 1.0 False {}\n",
      "[-0.04132994 -0.00531737 -0.12264455 -0.77881016] 1.0 False {}\n",
      "[-0.04143629  0.19125823 -0.13822075 -1.1074259 ] 1.0 False {}\n",
      "[-0.03761112  0.38789933 -0.16036927 -1.44008203] 1.0 False {}\n",
      "[-0.02985314  0.58459192 -0.18917091 -1.77828182] 1.0 False {}\n",
      "[-0.0181613   0.78127356 -0.22473655 -2.12332635] 1.0 True {}\n",
      "Episode finished after 33 steps\n",
      "[-0.02735463  0.1525351   0.03188406 -0.26224395] 1.0 False {}\n",
      "[-0.02430393  0.34718776  0.02663918 -0.54470221] 1.0 False {}\n",
      "[-0.01736017  0.15170181  0.01574514 -0.24374632] 1.0 False {}\n",
      "[-0.01432613 -0.04364146  0.01087021  0.05386114] 1.0 False {}\n",
      "[-0.01519896  0.15132295  0.01194744 -0.23537243] 1.0 False {}\n",
      "[-0.0121725  -0.04396764  0.00723999  0.06105502] 1.0 False {}\n",
      "[-0.01305186  0.15104976  0.00846109 -0.22933488] 1.0 False {}\n",
      "[-0.01003086  0.34604979  0.00387439 -0.51933692] 1.0 False {}\n",
      "[-0.00310987  0.54111698 -0.00651235 -0.81079644] 1.0 False {}\n",
      "[ 0.00771247  0.73632754 -0.02272828 -1.10552069] 1.0 False {}\n",
      "[ 0.02243902  0.93174085 -0.04483869 -1.40524649] 1.0 False {}\n",
      "[ 0.04107384  1.12738988 -0.07294362 -1.71160322] 1.0 False {}\n",
      "[ 0.06362164  0.93317752 -0.10717568 -1.44248577] 1.0 False {}\n",
      "[ 0.08228519  0.73952587 -0.1360254  -1.18512436] 1.0 False {}\n",
      "[ 0.09707571  0.9361243  -0.15972789 -1.51716624] 1.0 False {}\n",
      "[ 0.11579819  0.74325414 -0.19007121 -1.27830426] 1.0 False {}\n",
      "[ 0.13066328  0.94022119 -0.2156373  -1.62397907] 1.0 True {}\n",
      "Episode finished after 16 steps\n",
      "[ 0.02506262 -0.17510504 -0.012608    0.2854199 ] 1.0 False {}\n",
      "[ 0.02156052 -0.37004492 -0.0068996   0.57409984] 1.0 False {}\n",
      "[ 0.01415963 -0.56506947  0.0045824   0.86460122] 1.0 False {}\n",
      "[ 0.00285824 -0.76025349  0.02187442  1.15872141] 1.0 False {}\n",
      "[-0.01234683 -0.95565356  0.04504885  1.45818196] 1.0 False {}\n",
      "[-0.03145991 -0.76111226  0.07421249  1.17990593] 1.0 False {}\n",
      "[-0.04668215 -0.56702808  0.09781061  0.91137944] 1.0 False {}\n",
      "[-0.05802271 -0.37335596  0.1160382   0.65097027] 1.0 False {}\n",
      "[-0.06548983 -0.56988654  0.1290576   0.9778224 ] 1.0 False {}\n",
      "[-0.07688756 -0.37670905  0.14861405  0.72830367] 1.0 False {}\n",
      "[-0.08442174 -0.18391985  0.16318012  0.48583934] 1.0 False {}\n",
      "[-0.08810014  0.00856887  0.17289691  0.24870422] 1.0 False {}\n",
      "[-0.08792876  0.20085432  0.17787099  0.01515518] 1.0 False {}\n",
      "[-0.08391168  0.00368659  0.1781741   0.35825935] 1.0 False {}\n",
      "[-0.08383794 -0.19346159  0.18533929  0.70140648] 1.0 False {}\n",
      "[-0.08770718 -0.39060281  0.19936742  1.04623222] 1.0 False {}\n",
      "[-0.09551923 -0.19860446  0.22029206  0.82216583] 1.0 True {}\n",
      "Episode finished after 16 steps\n",
      "[ 0.0262256   0.14973471  0.0355333  -0.26440857] 1.0 False {}\n",
      "[ 0.02922029  0.34433194  0.03024513 -0.54567557] 1.0 False {}\n",
      "[ 0.03610693  0.53901616  0.01933162 -0.82867762] 1.0 False {}\n",
      "[ 0.04688725  0.73386856  0.00275807 -1.11521847] 1.0 False {}\n",
      "[ 0.06156462  0.92895419 -0.0195463  -1.40703494] 1.0 False {}\n",
      "[ 0.08014371  1.12431318 -0.047687   -1.70576369] 1.0 False {}\n",
      "[ 0.10262997  1.31995015 -0.08180227 -2.01290021] 1.0 False {}\n",
      "[ 0.12902897  1.12576801 -0.12206028 -1.74662405] 1.0 False {}\n",
      "[ 0.15154433  1.32204785 -0.15699276 -2.07464997] 1.0 False {}\n",
      "[ 0.17798529  1.51837679 -0.19848576 -2.41148819] 1.0 False {}\n",
      "[ 0.20835283  1.32545738 -0.24671552 -2.18576306] 1.0 True {}\n",
      "Episode finished after 10 steps\n",
      "[ 0.00529017 -0.19622453 -0.03974529  0.27773348] 1.0 False {}\n",
      "[ 0.00136568 -0.00055876 -0.03419062 -0.02721541] 1.0 False {}\n",
      "[ 0.0013545   0.1950364  -0.03473493 -0.33048677] 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00525523  0.39063514 -0.04134467 -0.63391792] 1.0 False {}\n",
      "[ 0.01306793  0.58630869 -0.05402302 -0.93932929] 1.0 False {}\n",
      "[ 0.02479411  0.39195499 -0.07280961 -0.66409909] 1.0 False {}\n",
      "[ 0.03263321  0.19791738 -0.08609159 -0.39520093] 1.0 False {}\n",
      "[ 0.03659155  0.39414866 -0.09399561 -0.71373738] 1.0 False {}\n",
      "[ 0.04447453  0.59043755 -0.10827036 -1.03446503] 1.0 False {}\n",
      "[ 0.05628328  0.78681955 -0.12895966 -1.35908248] 1.0 False {}\n",
      "[ 0.07201967  0.59352879 -0.15614131 -1.10936304] 1.0 False {}\n",
      "[ 0.08389025  0.79031889 -0.17832857 -1.44667642] 1.0 False {}\n",
      "[ 0.09969662  0.59778165 -0.2072621  -1.21460174] 1.0 False {}\n",
      "[ 0.11165226  0.40584628 -0.23155413 -0.9933601 ] 1.0 True {}\n",
      "Episode finished after 13 steps\n",
      "[ 0.02061312 -0.1478299   0.02147444  0.3210386 ] 1.0 False {}\n",
      "[ 0.01765652 -0.34325098  0.02789521  0.62041563] 1.0 False {}\n",
      "[ 0.0107915  -0.53875118  0.04030352  0.92175197] 1.0 False {}\n",
      "[ 1.64778375e-05 -7.34393879e-01  5.87385614e-02  1.22682373e+00] 1.0 False {}\n",
      "[-0.0146714  -0.93022071  0.08327504  1.53731659] 1.0 False {}\n",
      "[-0.03327581 -1.12624041  0.11402137  1.8547818 ] 1.0 False {}\n",
      "[-0.05580062 -1.32241579  0.151117    2.18058379] 1.0 False {}\n",
      "[-0.08224894 -1.51864836  0.19472868  2.51583762] 1.0 False {}\n",
      "[-0.11262191 -1.3255831   0.24504543  2.28860217] 1.0 True {}\n",
      "Episode finished after 8 steps\n",
      "[-0.02200375 -0.15834726 -0.03894586  0.31032647] 1.0 False {}\n",
      "[-0.0251707  -0.35289331 -0.03273933  0.59047708] 1.0 False {}\n",
      "[-0.03222856 -0.54754194 -0.02092979  0.87266992] 1.0 False {}\n",
      "[-0.0431794  -0.35214173 -0.00347639  0.57348088] 1.0 False {}\n",
      "[-0.05022224 -0.15697121  0.00799322  0.27970481] 1.0 False {}\n",
      "[-0.05336166  0.03803581  0.01358732 -0.01044639] 1.0 False {}\n",
      "[-0.05260095 -0.15727834  0.01337839  0.28649234] 1.0 False {}\n",
      "[-0.05574651  0.03765029  0.01910824 -0.00194131] 1.0 False {}\n",
      "[-0.05499351 -0.15774042  0.01906941  0.29670873] 1.0 False {}\n",
      "[-0.05814832  0.03710457  0.02500359  0.01010045] 1.0 False {}\n",
      "[-0.05740622  0.23185918  0.0252056  -0.27458986] 1.0 False {}\n",
      "[-0.05276904  0.03638684  0.0197138   0.02593518] 1.0 False {}\n",
      "[-0.0520413   0.23122061  0.0202325  -0.26046321] 1.0 False {}\n",
      "[-0.04741689  0.42604798  0.01502324 -0.5466965 ] 1.0 False {}\n",
      "[-0.03889593  0.62095567  0.00408931 -0.83460838] 1.0 False {}\n",
      "[-0.02647682  0.42577809 -0.01260286 -0.54064221] 1.0 False {}\n",
      "[-0.01796126  0.23083553 -0.0234157  -0.25195674] 1.0 False {}\n",
      "[-0.01334455  0.03605564 -0.02845484  0.03324942] 1.0 False {}\n",
      "[-0.01262343 -0.15864695 -0.02778985  0.31682048] 1.0 False {}\n",
      "[-0.01579637 -0.35336228 -0.02145344  0.60061154] 1.0 False {}\n",
      "[-0.02286362 -0.15794688 -0.00944121  0.30124906] 1.0 False {}\n",
      "[-0.02602255  0.03730836 -0.00341623  0.00560359] 1.0 False {}\n",
      "[-0.02527639 -0.15776443 -0.00330416  0.2972067 ] 1.0 False {}\n",
      "[-0.02843168  0.03740446  0.00263998  0.00348353] 1.0 False {}\n",
      "[-0.02768359 -0.15775525  0.00270965  0.29699823] 1.0 False {}\n",
      "[-0.03083869  0.03732797  0.00864961  0.00517111] 1.0 False {}\n",
      "[-0.03009213 -0.15791695  0.00875303  0.3005705 ] 1.0 False {}\n",
      "[-0.03325047 -0.35316256  0.01476444  0.59600105] 1.0 False {}\n",
      "[-0.04031372 -0.15825032  0.02668447  0.30800514] 1.0 False {}\n",
      "[-0.04347873  0.03648145  0.03284457  0.02385579] 1.0 False {}\n",
      "[-0.0427491  -0.15909575  0.03332168  0.32671793] 1.0 False {}\n",
      "[-0.04593102 -0.35467588  0.03985604  0.62972003] 1.0 False {}\n",
      "[-0.05302453 -0.55033068  0.05245044  0.93468373] 1.0 False {}\n",
      "[-0.06403115 -0.35595397  0.07114412  0.65893299] 1.0 False {}\n",
      "[-0.07115023 -0.16189053  0.08432278  0.38947294] 1.0 False {}\n",
      "[-0.07438804  0.03193969  0.09211224  0.12452217] 1.0 False {}\n",
      "[-0.07374924  0.22562956  0.09460268 -0.13773824] 1.0 False {}\n",
      "[-0.06923665  0.41927817  0.09184792 -0.39914058] 1.0 False {}\n",
      "[-0.06085109  0.22298146  0.0838651  -0.07897129] 1.0 False {}\n",
      "[-0.05639146  0.41680729  0.08228568 -0.34406075] 1.0 False {}\n",
      "[-0.04805531  0.61066813  0.07540446 -0.6097034 ] 1.0 False {}\n",
      "[-0.03584195  0.41457761  0.06321039 -0.29425552] 1.0 False {}\n",
      "[-0.0275504   0.60874409  0.05732528 -0.5663521 ] 1.0 False {}\n",
      "[-0.01537552  0.41286679  0.04599824 -0.25617437] 1.0 False {}\n",
      "[-0.00711818  0.21711931  0.04087476  0.05065498] 1.0 False {}\n",
      "[-0.00277579  0.41163205  0.04188785 -0.22885659] 1.0 False {}\n",
      "[0.00545685 0.21593732 0.03731072 0.07673945] 1.0 False {}\n",
      "[0.00977559 0.0203009  0.03884551 0.38095665] 1.0 False {}\n",
      "[ 0.01018161 -0.1753505   0.04646465  0.68563006] 1.0 False {}\n",
      "[0.0066746  0.01909665 0.06017725 0.40792983] 1.0 False {}\n",
      "[0.00705653 0.21331601 0.06833584 0.13480955] 1.0 False {}\n",
      "[0.01132285 0.01728516 0.07103203 0.44824463] 1.0 False {}\n",
      "[ 0.01166856 -0.1787659   0.07999693  0.76244551] 1.0 False {}\n",
      "[0.00809324 0.01516837 0.09524584 0.49596844] 1.0 False {}\n",
      "[ 0.00839661 -0.18115855  0.10516521  0.81708401] 1.0 False {}\n",
      "[ 0.00477344 -0.37755091  0.12150689  1.14090663] 1.0 False {}\n",
      "[-0.00277758 -0.57403321  0.14432502  1.4690923 ] 1.0 False {}\n",
      "[-0.01425825 -0.38094189  0.17370686  1.22475102] 1.0 False {}\n",
      "[-0.02187708 -0.18842919  0.19820188  0.99114107] 1.0 False {}\n",
      "[-0.02564567 -0.38557177  0.21802471  1.33895611] 1.0 True {}\n",
      "Episode finished after 59 steps\n",
      "[ 0.04562207  0.17289828  0.04514893 -0.24456347] 1.0 False {}\n",
      "[ 0.04908004 -0.02283848  0.04025766  0.06201176] 1.0 False {}\n",
      "[ 0.04862327 -0.21851382  0.0414979   0.36711951] 1.0 False {}\n",
      "[ 0.04425299 -0.41420013  0.04884029  0.67259315] 1.0 False {}\n",
      "[ 0.03596899 -0.21978987  0.06229215  0.39567883] 1.0 False {}\n",
      "[ 0.03157319 -0.4157378   0.07020573  0.70733271] 1.0 False {}\n",
      "[ 0.02325844 -0.61175848  0.08435238  1.02126294] 1.0 False {}\n",
      "[ 0.01102327 -0.80789683  0.10477764  1.3391946 ] 1.0 False {}\n",
      "[-0.00513467 -0.61423857  0.13156153  1.08104859] 1.0 False {}\n",
      "[-0.01741944 -0.42107537  0.1531825   0.83237529] 1.0 False {}\n",
      "[-0.02584095 -0.6179215   0.16983001  1.16904676] 1.0 False {}\n",
      "[-0.03819938 -0.42536572  0.19321095  0.93405875] 1.0 False {}\n",
      "[-0.04670669 -0.62249492  0.21189212  1.28070177] 1.0 True {}\n",
      "Episode finished after 12 steps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "#define a random agent for the game\n",
    "def random_agent(episodes, steps):\n",
    "    \n",
    "    #make a new game\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        #for every episode reset the game\n",
    "        observation = env.reset()\n",
    "        for i in range(steps):\n",
    "            #env.render()\n",
    "            #get a random action from both possible actions\n",
    "            action = env.action_space.sample()\n",
    "            #do action and get new state, reward, game over info and info\n",
    "            observation, reward, game_over, info = env.step(action)\n",
    "            print(observation, reward, game_over, info)\n",
    "            #check if game over\n",
    "            if game_over:\n",
    "                print('Episode finished after {} steps'.format(i))\n",
    "                break\n",
    "    env.close()\n",
    "\n",
    "random_agent(20, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "The replay buffer should store the last $N$ experience tuples. This is basically a FIFO queue and practically, [python offers such a data structure called](https://docs.python.org/3/library/collections.html#collections.deque) `deque`. If initialized with a `maxlen` parameter, `deque`'s `append` method will pop items from the left automatically when the list grows beyond the given `maxlen`. This is exactly what we want and **you can implement it in just a few lines of code!** The replay buffer should have the following methods:\n",
    "\n",
    "- `__init__` constructor, initializing an internal `deque` with a given `maxlen` or $N$ or better, we call it `buffer_size`.\n",
    "- `add` method, appending a new `[state, action, reward, next_state, done]` tuple. (`done` is the `game_over` information) \n",
    "- `sample` method, sample a random batch of training data of size `batch_size`. You can use `random`'s `sample` method for that.\n",
    "\n",
    "We use the cells below to test our implementation, e.g. by filling it with some integers in a loop, check whats in the queue and test the sampling method etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, float(done)))\n",
    "    def sample(self, batch_size):\n",
    "        return r.sample(self.memory,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 12, 13, 14, 15.0), (16, 17, 18, 19, 20.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING the replay buffer\n",
    "test_buffer = ReplayBuffer(2)\n",
    "test_buffer.add(1,2,3,4,5)\n",
    "test_buffer.add(6,7,8,9,10)\n",
    "test_buffer.add(11,12,13,14,15)\n",
    "test_buffer.add(16,17,18,19,20)\n",
    "test_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Schedule\n",
    "\n",
    "Last time, we calculated the current epsilon value inside the main loop. This time we need a little bit more control so let's create a class for that task. The reason for that is that we have to pre fill the replay buffer with some initial random experience before we can sample from it and start with the actual training. We want to control the amount of initial experience with a `pre_train_steps` variable. During this time, the schedule should return the `start_epsilon` value so that the agent behaves fully random. After that, the normal decay should be applied. The implementation needs two methods:\n",
    "\n",
    "- `__init__` constructor, takes all hyper parameters for the schedule such as `start_epsilon, final_epsilon, pre_train_steps, final_exploration_step`, pre calculate the decay value per step here.\n",
    "- `value` method, takes a time step `t` and returns a correpsonding `epsilon` value. If `t` is smaller or greater than the `pre_train_steps` or `final_exploration_step` return the fixed values accordingly. In between calculate the decayed `epsilon` value at time `t`.\n",
    "\n",
    "We use the code in the cell below to test and visualize your schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearSchedule():\n",
    " \n",
    "    def __init__(self, start_epsilon, final_epsilon, pre_train_steps, final_exploration_step):\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.final_exploration_step = final_exploration_step\n",
    "        #epsilon_decay is diffenrence of start and final epsilon divided by exploration steps\n",
    "        self.epsilon_decay = (start_epsilon - final_epsilon) / (final_exploration_step - pre_train_steps)\n",
    "        \n",
    "    #get epsilon after t steps\n",
    "    def value(self, t):\n",
    "        if t < self.pre_train_steps:\n",
    "            return self.start_epsilon\n",
    "        elif t > self.final_exploration_step:\n",
    "            return self.final_epsilon\n",
    "        else:\n",
    "            return self.start_epsilon - ((t-self.pre_train_steps)*self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x180a9f3ada0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TESTING the epsilon schedule\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#test schedule for start_epsilon = 1, final_epsilon=0.1, pre_train_steps=100, final_exploration_step=1000\n",
    "schedule    = LinearSchedule(1.0, 0.1, 100, 1000)\n",
    "test_points = [schedule.value(t) for t in range(1100)]\n",
    "\n",
    "plt.plot(test_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "\n",
    "The original DQN agent included a CNN as shown in the theory part of this notebook. For our task however, a simple MLP with only one hidden layer should be enough. Starting that simple will help us to get other implementation details right. Later on we can easily scale up and switch the MLP for a more powerfull network.\n",
    "\n",
    "While the architecture will be easy, the DQN algorithm requires us to keep basically two separate networks, namely, a main **Q-Network** and a second **Target-Network**. In addition, Tensorflow requires us to keep a reference to every node we want to calculate with the `sess.run` command. For those reasons we will build a *generic* and reusable `DQNetwork` class and bind all graph nodes to the object. Later we can then simply use the instance objects to reference specifc nodes in a clean and readable way.\n",
    "\n",
    "We will implement the `DQNetwork` class below in two steps. The first part denoted as *basic Deep Q-Network*, includes the actual network. The second part denote as *Q-Learning Calculations*, includes all the additional calculations to train the network.\n",
    "\n",
    "#### Part 1 - Basic Deep Q-Network\n",
    "\n",
    "This part is identical for the main Q- and the Target-Network. **It includes all trainable variables of the graph!** The model should be a fully connected feedforward network with one hidden layer of size $64$ and **ReLU** activations. For the CartPole task, the resulting MLP will consequently be later of size `num_inputs=4, num_hidden=64, num_outputs=2`. For the generic `DQNetwork` class however, let the user specify those values as parameters.\n",
    "\n",
    "- Now create a `self.best_action` node which should take the output layer (or q-values) and **return the indice** of the maximum action value. We can use `tf.argmax` for that. We can query this node in the e-greedy action selection later.\n",
    "\n",
    "- Create another node, `self.max_q` which does the same thing but **returns the value** of the maximum action value. We can use `tf.reduce_max` for that. We will need this node for the calculation of the TD-target later.\n",
    "\n",
    "So far, everything should have been very straightforward.\n",
    "\n",
    "#### Part 2 - Q-Learning Calculations\n",
    "\n",
    "This part is different for the Q- and the Target-Network. **It includes all the necessary calculations for training the network.** The code skeleton below shows how to constrain the graph creation with two simple if statements. \n",
    "\n",
    "First, the **Target Stream**. This part will later calculate the **TD-Target** (or $y_i$) with the following equation:\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "y_i = r_{t+1} + \\gamma \\max_a \\underbrace{Q(s_{t+1}, a; \\theta^{-}_{i})}_{Target-Network}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Remember that we will train with mini-batches sampled from the replay buffer. For that reason, the `reward` values will be provided by mini-batches. This means we have to feed them externally, so let's create a **placeholder node** for those. Next, gamma will be **a constant** so let's create one in tensorflow and let the user specify it as a parameter at creation time of the network. Next, $s_{t+1}$ are the `next_states` from the buffer. This is not important here but we have to feed them correctly later! To get the maximum action value, use the `self.max_q` node that we have created earlier. Finally, there is a small detail we have not talked about yet. **In the rare case that the next state is a final state, i.e the game is over, only the reward should be taken into account.** To realize this here is a little *trick*: create another placeholder for the boolean `done` values from the mini-batch. Then, multiply the right-hand side of the equation with `tf.abs(self.done - 1)`. For clarity call the final node which implements the equation above `self.td_target`! \n",
    "\n",
    "> Note that this is a *lot* of text but basically 4 lines of code!\n",
    "\n",
    "Second, the **Q Stream**. This part will calculate the full **TD-Error** and optimize the mean squared error on the mini-batch.\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "L_i(\\theta_{i}) = \\mathbb{E}_{(s,a,r,s') \\sim U(\\mathcal{D})} \\Bigg[ \\Bigg( y_i - \\underbrace{Q(s_t, a_t;\\theta_i)}_{Q-Network} \\Big)^2 \\Bigg]\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Now first, we create a placeholder for the **td-target** ($y_i$). These values have been calculated by the Target-Network and we must feed them again to the main Q-Network. Yes, its really two **separate** networks even though they share some code here. Next we need the value of $Q(s_t, a_t;\\theta_i)$. We can query **all action values** by feeding the mini-batch of `states` ($s_t$) from the buffer to the network (first part of the network). The *problem* is that **we only want to select the action value of the action that was actually taken**. Luckily we have this information as part of the mini-batch. We can feed it with an additional placeholder, let's call it `self.actions`. The idea is now to mask the output of the network with a one-hot encoded representation of the action indices from the placeholder. To do this use `tf.one_hot` and call the resulting node `self.actions_onehot` or similar. Finally multiply the one_hot vector with the output of the network and call `tf.reduce_sum` on the result to obtain a clean list of the action values we want. The resulting node, let's call it `self.Q` holds the values of $Q(s_t, a_t;\\theta_i)$ for the complete mini-batch.\n",
    "\n",
    "> If you feel uncomfortable with this *trick* create a toy example in a separate cell in order to understand whats going on here.\n",
    "\n",
    "Great, now the rest should be straightforward again. Square the difference of `td_target` and `Q` to obtain the **TD-error** and use `tf.reduce_mean` on that to obtain the expected **loss** of the mini-batch. This corresponds to $\\mathbb{E}_{(s,a,r,s') \\sim U(\\mathcal{D})}$ in the formula. Create an optimizer node such as the AdamOptimizer and again, let the user specifiy the learnin rate as a parameter at creation time of the network. Finally minimize the loss and call this final node something like `self.updateModel` or `self.train`.\n",
    "\n",
    "> Note that this is again a *lot* of text but *only* something like ~8 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "#build a generic and reusable DQNetwork class and bind all graph nodes to the object\n",
    "class DQNetwork():\n",
    "    \n",
    "    def __init__(self, scope, num_inputs=4, num_hidden=64, num_outputs=2, gamma=0.9, learnrate=0.5):\n",
    "        \n",
    "        self.scope = scope\n",
    "        \n",
    "        with tf.variable_scope(self.scope, reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            self.scope = scope\n",
    "            self.num_inputs = num_inputs\n",
    "            self.num_hidden = num_hidden\n",
    "            self.num_outputs = num_outputs\n",
    "            self.gamma = gamma\n",
    "            \n",
    "            # ---------------------\n",
    "            # Basic Deep Q-Network\n",
    "            # ---------------------\n",
    "            \"\"\"\n",
    "            This part is identical for the main Q- and the Target-Network. \n",
    "            It includes all trainable variables of the graph! \n",
    "            The model should be a fully connected feedforward network with one hidden layer of size 64 and ReLU activations. \n",
    "            For the CartPole task, the resulting MLP will consequently be later of size num_inputs=4, num_hidden=64, num_outputs=2.\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # Define variables\n",
    "            self.states = tf.placeholder(shape=[None, num_inputs], dtype=tf.float32)\n",
    "            # create a fully connected hidden layer\n",
    "            fc1 = tf.layers.dense(inputs=self.states, units=num_hidden, activation=tf.nn.relu)\n",
    "            self.logits = tf.layers.dense(inputs=fc1, units=num_outputs)\n",
    "            \"\"\"\n",
    "            #placeholder for input:\n",
    "            self.input_placeholder = tf.placeholder(tf.float32, shape=[None, num_inputs], name='input_placeholder')\n",
    "            #wheigts for hiddenLayer and outputLayer\n",
    "            self.BW1 = tf.get_variable(\"BW1\",shape= (self.num_inputs, self.num_hidden), initializer=tf.glorot_normal_initializer)\n",
    "            self.BB1 = tf.get_variable(\"BB1\",shape=(self.num_hidden), initializer=tf.zeros_initializer)\n",
    "            self.BW2 = tf.get_variable(\"BW2\",shape= (self.num_hidden, self.num_outputs), initializer=tf.glorot_normal_initializer)\n",
    "            self.BB2 = tf.get_variable(\"BB2\",shape=(self.num_outputs), initializer=tf.zeros_initializer)\n",
    "            #hiddenLayer\n",
    "            self.denseB = tf.matmul(self.input_placeholder, self.BW1) + self.BB1\n",
    "            self.denseB = tf.nn.relu(self.denseB)\n",
    "            #outputLayer\n",
    "            self.outB = tf.matmul( self.denseB, self.BW2) + self.BB2\n",
    "            #self.outB = tf.nn.relu(self.outB)\n",
    "            \n",
    "            \"\"\"\n",
    "            Now create a self.best_action node which should take the output layer (or q-values) and return the indice \n",
    "            of the maximum action value. \n",
    "            We can use tf.argmax for that. We can query this node in the e-greedy action selection later.\n",
    "            \"\"\"\n",
    "            #best_action index\n",
    "            self.best_action = tf.argmax(self.outB,axis=1)\n",
    "            \"\"\"\n",
    "            Create another node, self.max_q which does the same thing but returns the value of the maximum action value. \n",
    "            You can use tf.reduce_max for that. You will need this node for the calculation of the TD-target later.\n",
    "            \"\"\"\n",
    "            #return the value of the maximum action value\n",
    "            self.max_q = tf.reduce_max(self.outB, axis=1)\n",
    "            \n",
    "            # ------------------------\n",
    "            # Q-Learning Calculations\n",
    "            # ------------------------\n",
    "            \n",
    "            if scope == 'Target':\n",
    "                \"\"\"\n",
    "                Remember that we will train with mini-batches sampled from the replay buffer. \n",
    "                For that reason, the reward values will be provided by mini-batches. \n",
    "                This means we have to feed them externally, so let's create a placeholder node for those. \n",
    "                \"\"\"\n",
    "                self.rewards = tf.placeholder(dtype=tf.float32, name = 'rewards')\n",
    "                \n",
    "                \"\"\"\n",
    "                Next, gamma will be a constant so let's create one in tensorflow and let the user specify it \n",
    "                as a parameter at creation time of the network. \n",
    "                \"\"\"\n",
    "                \n",
    "                self.gamma = tf.constant(gamma)\n",
    "                \"\"\"\n",
    "                In the rare case that the next state is a final state, i.e the game is over, only the reward should be taken into account. \n",
    "                To realize this here is a little trick: create another placeholder for the boolean done values from the mini-batch. \n",
    "                \"\"\"\n",
    "                self.done = tf.placeholder(dtype=tf.float32, name = 'done')\n",
    "                \n",
    "                \"\"\"\n",
    "                Next, 𝑠𝑡+1 are the next_states from the buffer. This is not important here but you have to feed them correctly later! \n",
    "                To get the maximum action value, use the self.max_q node that you have created earlier. \n",
    "                Then, multiply the right-hand side of the equation with tf.abs(self.done - 1). \n",
    "                For clarity call the final node which implements the equation above self.td_target! \n",
    "                \"\"\"\n",
    "                self.td_target = tf.add(self.rewards, tf.multiply(tf.multiply(self.gamma, self.max_q),\\\n",
    "                                                                  tf.abs(tf.subtract(self.done, 1))),name = 'td_target' )\n",
    "                \n",
    "            if scope == 'Q':\n",
    "                #placeholder to feed td_target from target network.\n",
    "                self.td_target_placeholder = tf.placeholder(dtype=tf.float32, name = 'td_target_placeholder')\n",
    "                #placeholder to feed action value of the action that was actually taken\n",
    "                self.actions_placeholder = tf.placeholder(dtype=tf.int32, name = 'actions_placeholder')\n",
    "                # now mask the output of the network with a one-hot encoded representation of the action indices from the placeholder. \n",
    "                self.actions_onehot = tf.one_hot(self.actions_placeholder, depth=2)\n",
    "                #Finally multiply the one_hot vector with the output of the network \n",
    "                # obtain a clean list of the action values we want.\n",
    "                self.Q = tf.reduce_sum(tf.multiply(self.actions_onehot, self.outB), axis=1)\n",
    "                #TD-error\n",
    "                self.td_error = tf.square(tf.subtract(self.td_target_placeholder, self.Q))\n",
    "                self.loss = tf.reduce_mean(self.td_error)\n",
    "                \n",
    "                self.optimizer = tf.train.AdamOptimizer(learnrate)\n",
    "                self.train = self.optimizer.minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cell for testing\n",
    "testTarget = DQNetwork(\"Target\", 4, 64 , 2)\n",
    "testTarget = DQNetwork(\"Q\", 4, 64 , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check graph with tensorboard\n",
    "#writer = tf.summary.FileWriter('.')\n",
    "#writer.add_graph(tf.get_default_graph())\n",
    "#writer.flush()\n",
    "\n",
    "#tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Greedy Policy\n",
    "\n",
    "As in the Q-Learning notebook, let us encapsulate the action selection into a separate method. This time however, selecting a greedy max action requires us to perform a forward pass through the Q-Network. The method per se remains as simple as in the Q-Learning case. In order to perform the forward pass of the network though, we have to hand over a reference to the current Tensorflow session object from the main loop etc.\n",
    "\n",
    "Note that the `sess.run` call will most certainly return a list, containing a single action indice. Make sure to *unpack* it properly before returning it. We can test this in separate cell or in the main loop by using a **fixed epsilon value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#choose action greedily depending on state wheigts and epsilon \n",
    "def choose_egreedy_action(session, graph, state, epsilon):\n",
    "    #do random action with a probability corresponding to epsilon \n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(low=0, high=2) #between 0 and 1\n",
    "    else:\n",
    "        #run network to choose best_action and unpack the result\n",
    "        action = sess.run(graph.best_action, feed_dict={graph.input_placeholder : state})[0]\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Target Network\n",
    "\n",
    "As explained in the theory part, the Target-Network will be fixed for some time $C$ while the main Q-Network gets update every training/update step. Every $C$ time steps however we want to *switch* the networks or better, update the Target-Network with the latest information from the Q-Network. This basically means that we want to copy over all weights from the Q-Network and assign them to the Target-Network. The Q-Network itself remains unchanged. We will control this freeze frequency later inside the main loop and execute the copy process only every $C$ time steps.\n",
    "\n",
    "Assigning new values to variables in Tensorflow can be done with the `tf.assign` method. But, as everything in TensorFlow, these assign operations will be tensor nodes and **we have to create them before we start the session**. Since they don't belong to any of the networks let us do this in an extra method.\n",
    "\n",
    "> Note that in (before) the main loop we will have to create both networks first and then hand them to the `get_update_target_ops` method to obtain the list of assign operations!\n",
    "\n",
    "- get all trainable variables of a network with `tf.trainable_variables(scope=)`.\n",
    "- better sort the lists using `sorted` and the `attrgetter` helper, e.g `Q_vars = sorted(Q_vars, key=attrgetter('name'))`.\n",
    "- create an empty list for the assign expressions, let's call them something like `update_target_expr`.\n",
    "- loop over the variable lists and create assign opertations, e.g. `t_var.assign(q_var)`. Append them to the expression list.\n",
    "- a handy way for iterating over two lists is zip in a for loop. See the cell below for a little demo.\n",
    "- return the list of assign operations. We can later simply call `sess.run(update_target_expr)` to run all of the assign operations.\n",
    "\n",
    "\n",
    "Use the cells below to test our implementation with some toy networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "# Zip demo\n",
    "x = [1,2,3]\n",
    "y = [4,5,6]\n",
    "\n",
    "for x,y in zip(x,y):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "\n",
    "def get_update_target_ops(Q_network, Target_network):\n",
    "\n",
    "    # 1. get the trainable variables per network\n",
    "    #get all trainable variables of a network with tf.trainable_variables(scope=).\n",
    "    Q_trainable_vars = tf.trainable_variables(scope='Q')\n",
    "    Target_trainable_vars = tf.trainable_variables(scope='Target')\n",
    "\n",
    "    \n",
    "    # 2. sort them with sorted(list, key=attrgetter())\n",
    "    #better sort the lists using sorted and the attrgetter helper, e.g Q_vars = sorted(Q_vars, key=attrgetter('name')).\n",
    "    Q_trainable_vars = sorted(Q_trainable_vars, key=attrgetter('name'))\n",
    "    Target_trainable_vars = sorted(Target_trainable_vars, key=attrgetter('name'))\n",
    "    #print(\"Q_trainable_vars\", Q_trainable_vars)\n",
    "    #print(\"Target_trainable_vars\", Target_trainable_vars)\n",
    "\n",
    "    \n",
    "    # 3.create a new list with all assign ops\n",
    "    #create an empty list for the assign expressions, let's call them something like update_target_expr\n",
    "    update_target_expr = []\n",
    "    #loop over the variable lists and create assign opertations, e.g. t_var.assign(q_var).\n",
    "    #Append them to the expression list.\n",
    "    for Q_trainable_var, Target_trainable_var in zip(Q_trainable_vars, Target_trainable_vars):\n",
    "        update_target_expr.append(Target_trainable_var.assign(Q_trainable_var))\n",
    "    return update_target_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of created Assign operations\n",
      "[<tf.Tensor 'Assign:0' shape=(2,) dtype=float32_ref>, <tf.Tensor 'Assign_1:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'Assign_2:0' shape=(1, 2) dtype=float32_ref>, <tf.Tensor 'Assign_3:0' shape=(2, 1) dtype=float32_ref>]\n",
      "\n",
      " Q Variables\n",
      "[<tf.Variable 'Q/BW1:0' shape=(1, 2) dtype=float32_ref>, <tf.Variable 'Q/BB1:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'Q/BW2:0' shape=(2, 1) dtype=float32_ref>, <tf.Variable 'Q/BB2:0' shape=(1,) dtype=float32_ref>]\n",
      "\n",
      " Target Variables\n",
      "[<tf.Variable 'Target/BW1:0' shape=(1, 2) dtype=float32_ref>, <tf.Variable 'Target/BB1:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'Target/BW2:0' shape=(2, 1) dtype=float32_ref>, <tf.Variable 'Target/BB2:0' shape=(1,) dtype=float32_ref>]\n",
      "\n",
      " Q Values\n",
      "[array([[-0.31879127, -0.29016402]], dtype=float32), array([0., 0.], dtype=float32), array([[ 0.95279896],\n",
      "       [-0.52052546]], dtype=float32), array([0.], dtype=float32)]\n",
      "\n",
      " Target Values\n",
      "[array([[0.2528751 , 0.45456573]], dtype=float32), array([0., 0.], dtype=float32), array([[1.0818949 ],\n",
      "       [0.00552013]], dtype=float32), array([0.], dtype=float32)]\n",
      "\n",
      " Target Values AFTER network copy. Should now be identical to the values of Q\n",
      "[array([[-0.31879127, -0.29016402]], dtype=float32), array([0., 0.], dtype=float32), array([[ 0.95279896],\n",
      "       [-0.52052546]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING get_update_target_ops with some toy networks\n",
    "tf.reset_default_graph()\n",
    "Q = DQNetwork(\"Q\", 1,2,1) \n",
    "Target = DQNetwork(\"Target\", 1,2,1)\n",
    "\n",
    "Q_vars = tf.trainable_variables(scope=\"Q\")\n",
    "Target_vars = tf.trainable_variables(scope=\"Target\")\n",
    "update_target_expr = get_update_target_ops(Q, Target)\n",
    "\n",
    "print(\"List of created Assign operations\"), print(update_target_expr)\n",
    "print(\"\\n Q Variables\"), print(Q_vars)\n",
    "print(\"\\n Target Variables\"), print(Target_vars)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())        \n",
    "\n",
    "    print(\"\\n Q Values\"), print(sess.run(Q_vars))\n",
    "    print(\"\\n Target Values\"), print(sess.run(Target_vars))\n",
    "    \n",
    "    sess.run(update_target_expr)\n",
    "\n",
    "    print(\"\\n Target Values AFTER network copy. Should now be identical to the values of Q\")\n",
    "    print(sess.run(Target_vars))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Target Values are equal to Q Values AFTER network copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Method\n",
    "\n",
    "Finally let's create a method to run the actual network training. Doing this in an extra method is not necessary per se but un-clutters the main loop a lot. The implementation requires the following three steps:\n",
    "\n",
    "1. Sample a mini-batch of experience from the replay buffer. In order to feed everything to the DQNetworks, the mixed batch must be reshaped into separate **list** batches (`[]`!) of `observations, actions, rewards, next_observations` and `done`. Make sure this true before creating corresponding feed_dicts! Depending on your replay buffer implementation you way find it handy to use `zip` again and apply the `list` operator with the `map` function, e.g. `map(list, zip(*batch))`. See the cell below for an UnZip demo.\n",
    "2. Prepare an appropriate `feed_dict` and get the `td_target` values by running the Target-Network.\n",
    "3. Prepare an appropriate `feed_dict` and run the `update_model` or `train` operation of the main Q-Network.\n",
    "\n",
    "Now we can simply call `train` inside the main loop every time we want to train the Q-Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "(2, 2, 2)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# UnZip demo *\n",
    "mini_batch = [[1,2,3], [1,2,3], [1,2,3]]\n",
    "    \n",
    "for i in zip(*mini_batch):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reminder: replaybuffer order : state, action, reward, next_state, done\n",
    "\n",
    "def train(sess, Q, Target, buffer, batch_size):\n",
    "    # 1. Sample from the replay buffer:\n",
    "    #Sample a mini-batch of experience from the replay buffer. In order to feed everything to the DQNetworks, \n",
    "    #the mixed batch must be reshaped into separate list batches ([]!) of \n",
    "    #observations, actions, rewards, next_observations and done. \n",
    "    #Make sure this true before creating corresponding feed_dicts! \n",
    "    #Depending on your replay buffer implementation you way find it handy to use \n",
    "    #zip again and apply the list operator with the map function, e.g. map(list, zip(*batch)).\n",
    "    #See the cell below for an UnZip demo.\n",
    "    mini_batch = buffer.sample(batch_size)\n",
    "    mini_batch = list(map(list, zip(*mini_batch)))\n",
    "    #print(mini_batch[0])\n",
    "    #print(mini_batch[1])\n",
    "    #print(mini_batch[2])\n",
    "    #print(mini_batch[3])\n",
    "    #print(mini_batch[4])\n",
    "\n",
    "    \n",
    "    # 2. Retrieve the TD-target values from the Target-Network\n",
    "    #Prepare an appropriate feed_dict and get the td_target values by running the Target-Network.\n",
    "    # we feed next_state, reward and done\n",
    "    td_target = sess.run(Target.td_target, feed_dict = {Target.input_placeholder: mini_batch[3], Target.rewards:mini_batch[2],\\\n",
    "                                  Target.done:mini_batch[4]})\n",
    "    \n",
    "    # 3. Perform a training step by running the main Q-Network\n",
    "    # we feed action and state\n",
    "    sess.run(Q.train, feed_dict = {Q.td_target_placeholder: td_target, Q.actions_placeholder: mini_batch[1],\\\n",
    "                                   Q.input_placeholder: mini_batch[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop\n",
    "\n",
    "Below we can see the **Deep Q-Learning** pseudo code from the original paper. This will help us to get at least the main parts of the algorithm right and should serve as a rough blue print of when to do what inside the loop. In practice, there are many more subtle details which are not mentioned explicitly.\n",
    "\n",
    "\n",
    "---\n",
    "##### Deep Q-Learning with experience replay\n",
    "\n",
    "- Initialize replay memory $D$ to capacity $N$\n",
    "- Initialize action-value function $Q$ with random weights $\\theta$\n",
    "- Initialize target action-value function $\\hat{Q}$ with random weights $\\theta^{-}$\n",
    "\n",
    "- **For** $t = 1, T$ **do**\n",
    "    - With probability $\\epsilon$ select a random action $a_t$\n",
    "    - otherwise select $a_t = \\text{arg}\\max_a Q(s_t,a;\\theta)$\n",
    "    <br><br>\n",
    "    - Execute action $a_t$ in emulator and observe reward $r_t$ and state $s_{t+1}$\n",
    "    - Store transition $(s_t,a_t,r_t,s_{t+1})$ in $D$\n",
    "    - Sample random minibatch transitions $(s_j,a_j,r_j,s_{j+1})$ from $D$\n",
    "    - set    \n",
    "    $\n",
    "    y_j = \\begin{cases}\n",
    "          r_j  & \\text{if episode terminates at step } j + 1 \\\\               \n",
    "          r_j + \\gamma \\max_a \\hat{Q}(s_{j+1}, a; \\theta^{-})  & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "$\n",
    "    \n",
    "    - Perform a gradient descent step on $\\big(y_j - Q(s_j,a_j;\\theta)\\big)^2$ with respect to the network parameters $\\theta$\n",
    "    - Every $C$ steps reset $\\hat{Q} = Q$\n",
    "- **End For**\n",
    "---\n",
    "#####  Deep Q-Learning - PIA checklist \n",
    "\n",
    "> This is a helpful checklist:\n",
    "\n",
    "**Preparation and hyper parameters**\n",
    "\n",
    "- Epsilon Schedule\n",
    "    - `start_epsilon` $1$\n",
    "    - `final_epsilon` $\\in \\{0.02,0.1\\}$ \n",
    "    - `pre_training_steps` $\\sim[32,...,10000]$\n",
    "    - `final_exploration_step` $\\sim [100,...,40000]$\n",
    "\n",
    "- Replay Buffer\n",
    "    - `buffer_size` $N \\in \\{32,100,500,1000,10000,50000, ...?\\}$ (Bigger is better but try small ones too!)\n",
    "\n",
    "- Training\n",
    "    - total/max time steps `T` $\\in \\{10k,20k,30k,40k,100k\\}$\n",
    "    - `training_freq` $1$ - train the Q-Network only every $n$ steps. For now just use 1 as default.\n",
    "    - `switch_networks` $C \\sim 500$ \n",
    "    - `gamma` $\\in \\{0.9, 0.99, 1\\}$\n",
    "    - `batch_size` $32$\n",
    "    - `learning_rate` $0.001$\n",
    "    \n",
    "- Model\n",
    "    - always call `tf.reset_default_graph()` before creating a new graph\n",
    "    - get the `observation_space` and `action_space` from the game env\n",
    "    - `num_hidden` $64$\n",
    "    - create a `Q_network` and `T_network` with scope `\"Q\"` and `\"Target\"`\n",
    "    - save the result from `get_update_target_ops` to something like `update_target_network`\n",
    "    \n",
    "**Inside the Loop**\n",
    "\n",
    "- Use the pseudo code as a guideline\n",
    "- Remember to train and switch the networks only after `t` becomes `t > pre_training_steps`\n",
    "- Maybe obvious but remember to set `observation = new_observation` for $t+1$\n",
    "\n",
    "**If the Loop runs without errors**\n",
    "\n",
    "- Create insight\n",
    "    - keep track of the episode rewards, calculate a 10 mean, 100 mean\n",
    "    - print some info every $n'th \\sim 2000$ time step, e.g. current step, epsilon, mean reward etc.\n",
    "    - plot the epsilon schedule vs. the reward\n",
    "   \n",
    "- Save/Load the model - needed for the test evaluation later\n",
    "    - `saver = tf.train.Saver()` - outside the session\n",
    "    - `saver.save(sess, \"./some_path/model.ckpt\")` - with an active session\n",
    "    - `saver.restore(sess, \"./some_path/model.ckpt\")` - with a new session. In this case `tf.global_variables_initializer()` is not required. A fitting graph definition however is. If there is none left in ram, e.g. if the kernel was restartet, make sure you recreate the graph definition before restoring (at least the main Q-Network). You will need it anyway to reference nodes in the `sess.run` calls later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "N=5000 \n",
    "learnrate = 0.001\n",
    "gamma = 0.99\n",
    "T=40000\n",
    "start_epsilon=1\n",
    "final_epsilon=0.0\n",
    "pre_train_steps=1000\n",
    "final_exploration_step=30000\n",
    "batch_size = 32\n",
    "switch_network_frequency = 500 #C\n",
    "# Create a new game\n",
    "game = gym.make('CartPole-v0')\n",
    "state = game.reset()\n",
    "# Initialize replay memory  D  to capacity  N\n",
    "D= ReplayBuffer(N)\n",
    "#Initialize action-value function  Q  with random weights  θ\n",
    "Q_network = DQNetwork('Q', num_inputs=4, num_hidden=64, num_outputs=2, gamma=gamma, learnrate=learnrate)\n",
    "#Initialize target action-value function  Q̂   with random weights  θ−\n",
    "Target_network = DQNetwork('Target', num_inputs=4, num_hidden=64, num_outputs=2, gamma=gamma, learnrate=learnrate)\n",
    "#savemodel prep\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step:  2000 epsilon:  0.9655172413793104  reward_meanLast100:  19.68  reward_meanLast10:  19.1\n",
      "current step:  4000 epsilon:  0.896551724137931  reward_meanLast100:  24.92  reward_meanLast10:  23.6\n",
      "current step:  6000 epsilon:  0.8275862068965517  reward_meanLast100:  30.99  reward_meanLast10:  33.2\n",
      "current step:  8000 epsilon:  0.7586206896551724  reward_meanLast100:  38.9  reward_meanLast10:  40.6\n",
      "current step:  10000 epsilon:  0.6896551724137931  reward_meanLast100:  43.97  reward_meanLast10:  56.2\n",
      "current step:  12000 epsilon:  0.6206896551724137  reward_meanLast100:  50.2  reward_meanLast10:  64.5\n",
      "current step:  14000 epsilon:  0.5517241379310345  reward_meanLast100:  58.84  reward_meanLast10:  90.6\n",
      "current step:  16000 epsilon:  0.48275862068965514  reward_meanLast100:  72.21  reward_meanLast10:  99.2\n",
      "current step:  18000 epsilon:  0.4137931034482758  reward_meanLast100:  85.18  reward_meanLast10:  141.1\n",
      "current step:  20000 epsilon:  0.3448275862068966  reward_meanLast100:  98.99  reward_meanLast10:  176.4\n",
      "current step:  22000 epsilon:  0.27586206896551724  reward_meanLast100:  113.38  reward_meanLast10:  175.9\n",
      "current step:  24000 epsilon:  0.2068965517241379  reward_meanLast100:  126.82  reward_meanLast10:  177.4\n",
      "current step:  26000 epsilon:  0.13793103448275856  reward_meanLast100:  139.83  reward_meanLast10:  191.9\n",
      "current step:  28000 epsilon:  0.06896551724137923  reward_meanLast100:  151.86  reward_meanLast10:  200.0\n",
      "current step:  30000 epsilon:  0.0  reward_meanLast100:  164.62  reward_meanLast10:  200.0\n",
      "current step:  32000 epsilon:  0.0  reward_meanLast100:  171.07  reward_meanLast10:  200.0\n",
      "current step:  34000 epsilon:  0.0  reward_meanLast100:  181.08  reward_meanLast10:  198.4\n",
      "current step:  36000 epsilon:  0.0  reward_meanLast100:  188.02  reward_meanLast10:  186.1\n",
      "current step:  38000 epsilon:  0.0  reward_meanLast100:  188.34  reward_meanLast10:  178.0\n",
      "current step:  40000 epsilon:  0.0  reward_meanLast100:  190.28  reward_meanLast10:  183.3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epsilon_schedule = LinearSchedule(start_epsilon, final_epsilon, pre_train_steps, final_exploration_step)\n",
    "    episode_length=0\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    current_episode_reward=0\n",
    "    results=[]\n",
    "    #For  t=1,T  do\n",
    "    for t in range (1, T + 1):\n",
    "        episode_length+=1\n",
    "        #With probability  ϵ  select a random action  at \n",
    "        #otherwise select  at=argmaxaQ(st,a;θ)  \n",
    "        epsilon = epsilon_schedule.value(t)\n",
    "        at = choose_egreedy_action(sess, Target_network, [state], epsilon)\n",
    "        #Execute action  at  in emulator and observe reward  rt  and state  st+1 \n",
    "        #Store transition  (st,at,rt,st+1)  in  D \n",
    "        next_state, reward, game_over, info = game.step(at)\n",
    "        \n",
    "        #check if game_over --> reset\n",
    "        if game_over:\n",
    "            next_state = game.reset()\n",
    "            episode_rewards.append(current_episode_reward)\n",
    "            episode_length=0\n",
    "            current_episode_reward=0\n",
    "        #reminder: replaybuffer order : state, action, reward, next_state, done\n",
    "        D.add(state, at, reward,next_state,game_over)\n",
    "        current_episode_reward+=reward\n",
    "        state = next_state\n",
    "\n",
    "        if t > pre_train_steps:\n",
    "            #Sample random minibatch transitions  (sj,aj,rj,sj+1)  from  D \n",
    "            #set yj={rjrj+γmaxaQ̂ (sj+1,a;θ−)if episode terminates at step j+1otherwise \n",
    "            #Perform a gradient descent step on  (yj−Q(sj,aj;θ))2  with respect to the network parameters  θ \n",
    "            train(sess, Q_network, Target_network, D, batch_size)\n",
    "            #Every  C  steps reset  Q̂ =Q \n",
    "            if t % switch_network_frequency == 0:\n",
    "                #copy wheights\n",
    "                update_target_ops = get_update_target_ops(Q_network, Target_network)\n",
    "                sess.run(update_target_ops)\n",
    "        #if t > 30000:\n",
    "            #game.render()\n",
    "\n",
    "        #eval\n",
    "\n",
    "        if t % 2000 ==0:\n",
    "            reward_mean_last100 = sum(list(episode_rewards)[-100:]) / 100\n",
    "            reward_mean_last10 = sum(list(episode_rewards)[-10:]) / 10\n",
    "            print(\"current step: \", t,\"epsilon: \", epsilon, \" reward_meanLast100: \",reward_mean_last100,\\\n",
    "                  \" reward_meanLast10: \",reward_mean_last10)\n",
    "            #save data in results\n",
    "            results.append([t, epsilon, reward_mean_last100])    \n",
    "\n",
    "        #End For\n",
    "    #save model\n",
    "    saver.save(sess, \"./DPQmodel/modelDQL.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prep data(t, epsilon, reward_mean_last100) saved i results\n",
    "final = []\n",
    "for i in zip(*results):\n",
    "    final.append(i)\n",
    "#epsilon\n",
    "data1=list(final[1])\n",
    "reward_mean_last100\n",
    "data2=list(final[2])\n",
    "#t = T(=Number of total steps) /2000\n",
    "t=[x for x in range(1,T+1,2000)]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('steps (s)')\n",
    "ax1.set_ylabel('epsilon', color=color)\n",
    "ax1.plot(t, data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('mean reward', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Deep Reinforcement Learning Algorithms\n",
    "\n",
    "\n",
    "As part of this notebook, we will evaluate our algorithm as done by the authors of DQN. The testing is very simple. Let the trained agent play the game $30$ times with an e-greedy policy with a fixed $\\epsilon = 0.05$ and report the average high score (return).\n",
    "\n",
    "- Load a pre-trained agent, potentially recreate a Q-Network graph.\n",
    "- Run the agent for 30 episodes with an `evaluation_epsilon = 0.05`.\n",
    "- Plot or print the results in a decent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DPQmodel/modelDQL.ckpt\n",
      "Average reward after 30 episodes: 189.56666666666666\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "evaluation_episodes = 30\n",
    "evaluation_epsilon = 0.05\n",
    "num_hidden=64\n",
    "game = gym.make('CartPole-v0')\n",
    "state = game.reset()\n",
    "Q_network = DQNetwork('Q', num_inputs=4, num_hidden=num_hidden, num_outputs=2)\n",
    "rewardsum = 0\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore session\n",
    "    saver.restore(sess, \"./DPQmodel/modelDQL.ckpt\")\n",
    "    #play game for 'evaluation_episodes' episodes\n",
    "    for t in range (1, evaluation_episodes + 1):\n",
    "        game_over = False\n",
    "        while not game_over:        \n",
    "            at = choose_egreedy_action(sess, Q_network, [state], evaluation_epsilon)\n",
    "            next_state, reward, game_over, info = game.step(at)\n",
    "            state=next_state\n",
    "            #add up reward of current episode\n",
    "            rewardsum += reward\n",
    "        #print(\"rewardsum after epsiode\", t, \":\", rewardsum)\n",
    "        #reset game if game is over\n",
    "        state = game.reset()\n",
    "    game.close()\n",
    "    #get average reward of all episodes\n",
    "    print(\"Average reward after\", evaluation_episodes, \"episodes:\", rewardsum/evaluation_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
