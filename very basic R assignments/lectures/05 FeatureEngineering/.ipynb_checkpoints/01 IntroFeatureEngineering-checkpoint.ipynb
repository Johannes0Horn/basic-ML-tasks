{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Feature Engineering\n",
    "\n",
    "* Author: Oliver Kretzschmar\n",
    "* Last Update: 2019-05-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remember: Our Starting Point is Tidy Data\n",
    "\n",
    "A dataset in our context is \"tidy\", if it satisfies the following conditions\n",
    "\n",
    "* the observations are in the rows\n",
    "* the variables or features are in the columns\n",
    "* and contained in a single dataset.\n",
    "\n",
    "<html><img src=\"../img/TidyData1.jpg\", width=800></html>\n",
    "<font size=\"1\">(Imagesource: [WH17])</font>\n",
    "\n",
    "Tidy data is a one of the prerequisites for good machine learning results. To get tidy data we have to do a lot of \n",
    "things, e.g. data cleaning, feature engineering, selection, extraction and so on. The process to transform and mapping the raw data into tidy data format is also known as *data wrangling*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Feature Engineering?\n",
    "\n",
    "*Features* are known as the input variables of a data set. As we have seen in a former section, the part of preparing the data in a machine learning process is very important and mostly time-consuming.\n",
    "\n",
    "`What are we doing in Feature Engineering?` There are a lot of things to do, e.g. investigate the data by visualizing and other methods, handle missing values, cleaning and transforming features into formats so that they are suitable for our machine learning model and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`What are we doing in Feature Extraction and Selection?` Because of lot of machine learning models are very sensitive about wrong features (we discussed this in context of *domain knowledge*) - independent of computer time and other disadvantages, we have to find the *right* features for our use case and remove the other ones (`Discussion:` What means here *right*?) .\n",
    "\n",
    "After a short introduction to data, we will check out different methods and techniques of feature engineering, extraction and selection as described in [ZC18], [KJK19], [REF19] and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many Data do we need?\n",
    "\n",
    "We can not answer this question exactly, but we know some influencing factors:\n",
    "\n",
    "* Complexity of model\n",
    "* Variance of value distribution\n",
    "* Number of features and learning parameters\n",
    "* Influences between feature and target variable\n",
    "* Expected uniqueness of (inferential) statistical conclusions\n",
    "\n",
    "<font size=\"2\">(Source: Urban, Dieter ; Mayerl, Jochen: Angewandte Regressionsanalyse: Theorie, Technik und Praxis. 5. Springer VS, 2018)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common rule say that the necessary quantity of data is exponentially increasing, if the number of features increase - especially in Deep Learning. So for that reason in most situations it is necessary to do a carefully consideration of data as we have seen before - so called Data Cleaning, Feature Engineering, Selection, Extraction a.s.o. See also [CLO09]).\n",
    "    \n",
    "What can we do, if we have to low data?\n",
    "\n",
    "- Augmentation\n",
    "- Better Data Evaluation and Feature Engineering\n",
    "- Active Learning - careful selection of trainings samples\n",
    "- Apply pretrained models\n",
    "- Resampling methods\n",
    "<BR>\n",
    "    \n",
    "<font size=\"2\">(Source: Maucher, J.: Introduction to Artificial Intelligence, Machine Learning and Deep Learning, IHK Workshop, 2018)</font>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Influence of Data Scale Levels to Models\n",
    "\n",
    "Different scale levels of the variables:\n",
    "\n",
    "<html><img src=\"../img/ScaleLevels.jpg\"></html>\n",
    "<font size=\"2\">(Imagesource: [BEP10])</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The scale level of the variables (input/output) influence the set of possible applicable machine learning algorithm. For example:\n",
    "\n",
    "<html><img src=\"../img/MLAlgo4ScaleLevels.jpg\"></html>\n",
    "<font size=\"2\">(Imagesource: [BEP10])</font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "664px",
   "left": "0px",
   "right": "1209.67px",
   "top": "125.333px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
